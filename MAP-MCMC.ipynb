{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656c712c",
   "metadata": {},
   "source": [
    "**Start with your name(s) and student number(s)**\n",
    "\n",
    "Team:\n",
    "\n",
    "* FILL IN YOUR NAME (AND STUDENT NUMBER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc6f69c-e3b2-4658-86ef-e42a058448e2",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "\n",
    "In this notebook we will look at extracting different assignments from the PGMs in an efficient way. We do this in two parts:\n",
    "\n",
    "For the first part we want to extract the assignment that gives the highest probability (a.k.a. the mode) given certain conditionals and evidence. We do this through a Max-Product version of Variable Elimination (VE).\n",
    "\n",
    "For the second part we will extract assignments randomly, where the probability of extraction is based on the probability of the assignment in the model. This is just a complicated way to say that we will do sampling. We will learn how to sample efficiently from a PGM through the Gibbs sampling algorithm, a simple sampling algorithm that makes use of the graphical structure to avoid constructing large factors.\n",
    "\n",
    "We will use an example of reasoning w.r.t. a medical diagnosis to build up the intuition and the methods. We will apply the methods to a toy language generation example.\n",
    "\n",
    "This is a high-level outline of the notebook, you will find exercises in most sections.\n",
    "\n",
    "1. We start with constructing the BN (DAG + CPD) (similar to last week)\n",
    "2. We will implement and reason about naive (arg)max inference for probability queries\n",
    "3. Next, we will implement and reason about the Max-Product VE algorithm \n",
    "4. We will implement gibbs sampling and compare it to the exact case\n",
    "5. We will check whether convergence/mixing actually happens\n",
    "6. We will look at calculating expectations using samples\n",
    "\n",
    "**Table of Exercises**\n",
    "\n",
    "The exercises and the points they are worth are shown below. \n",
    "\n",
    "\n",
    "1. Max-Product Inference - Exhaustive [1]\n",
    "2. Max-Product Inference - Exhaustive - Conditional Query [1]\n",
    "3. Max-Product VE [1]\n",
    "4. Exact but Intractable Sampling (parts 1 and 2) [2]\n",
    "5. Gibbs sampling (parts 1 and 2) [2]\n",
    "6. Studying Convergence [2]\n",
    "7. Expectations [1]\n",
    "\n",
    "\n",
    "**Use of AI tools**\n",
    "\n",
    "In this course we expect _you_ and your team members to author your work.\n",
    "AI tools are not to be used for drafts, nor code completion, nor revisions, nor as a source of feedback. If you do use AI, it should not contribute to the substance of what you present as your work.  \n",
    "\n",
    "At the end of this notebook you will find a section on _Use of AI tools_. **Make sure to read and complete it**. \n",
    "By submitting a version of this notebook for assessment, you agree with our terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec6d4b",
   "metadata": {},
   "source": [
    "# Setting Up\n",
    "\n",
    "Take care of dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f15b8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabulate\n",
    "# !pip install --upgrade --force-reinstall  git+https://github.com/probabll/pgmini.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "892a302f-7769-4909-8fc1-8b99f232e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pgmini\n",
    "assert pgmini.__version__ == '0.4.0', \"Don't forget to update pgmini and restart your kernel\\n!pip install --upgrade --force-reinstall  git+https://github.com/probabll/pgmini.git\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "276f1c0e-d14f-45d9-9065-681ff81d66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmini.m1 import OutcomeSpace, DAG\n",
    "from pgmini.m2 import TabularFactor, UGraph\n",
    "from pgmini.m3 import TabularCPDFactor, PGM, BayesianNetwork, MarkovNetwork \n",
    "from pgmini.m4 import split_factors, sum_product_variable_elimination, max_product_variable_elimination, rhat_split\n",
    "from pgmini.util import display_full_table, pgm_to_tabular_factor, pgm_to_df\n",
    "from pgmini.util import make_samples_df, df_to_factor, tvd\n",
    "from joblib import Parallel, delayed\n",
    "import functools\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import urllib.request\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b0c5e-c147-46fd-80f3-66641e024895",
   "metadata": {},
   "source": [
    "**Misconception example**. This is a little helper that will be useful when demonstrating some of the functionalities in the notebook. It returns the Misconception example as we covered in class. \n",
    "\n",
    "The actual exercises will not be based on the Misconception example, instead they will be based on the Medical BN example, which we will construct afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8b66e5e7-8352-4d4b-a028-73716d39f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_misconception_example():\n",
    "    \"\"\"The Misconception MN from class\"\"\"\n",
    "    \n",
    "    misconception_outcome_spaces = {\n",
    "        'A': OutcomeSpace(['a0', 'a1']), \n",
    "        'B': OutcomeSpace(['b0', 'b1']),\n",
    "        'C': OutcomeSpace(['c0', 'c1']),\n",
    "        'D': OutcomeSpace(['d0', 'd1'])\n",
    "    }\n",
    "    \n",
    "    # Define factors\n",
    "    phi1 = TabularFactor(\n",
    "        ['A', 'B'],\n",
    "        misconception_outcome_spaces,\n",
    "        [[30, 5], [1, 10]]\n",
    "    )\n",
    "    phi2 = TabularFactor(\n",
    "        ['B', 'C'],\n",
    "        misconception_outcome_spaces,\n",
    "        [[100, 1], [1, 100]]\n",
    "    )\n",
    "    phi3 = TabularFactor(\n",
    "        ['C', 'D'],\n",
    "        misconception_outcome_spaces,\n",
    "        [[1, 100], [100, 1]]\n",
    "    )\n",
    "    phi4 = TabularFactor(\n",
    "        ['D', 'A'],\n",
    "        misconception_outcome_spaces,\n",
    "        [[100, 1], [1, 100]]\n",
    "    )\n",
    "        \n",
    "    return MarkovNetwork([phi1, phi2, phi3, phi4])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d96f5f-b119-41e6-9e3b-5073c911400e",
   "metadata": {},
   "source": [
    "You can view the misconception MN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "277e0ddc-4982-4884-a171-7e0141bd1303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| nodes and edges   |\n",
      "+===================+\n",
      "| A                 |\n",
      "+-------------------+\n",
      "| B                 |\n",
      "+-------------------+\n",
      "| C                 |\n",
      "+-------------------+\n",
      "| D                 |\n",
      "+-------------------+\n",
      "| A -- B            |\n",
      "+-------------------+\n",
      "| A -- D            |\n",
      "+-------------------+\n",
      "| B -- C            |\n",
      "+-------------------+\n",
      "| C -- D            |\n",
      "+-------------------+\n"
     ]
    }
   ],
   "source": [
    "misconception_mn = make_misconception_example()\n",
    "print(misconception_mn.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91029615-b4f2-4c18-a1a8-035a89af4359",
   "metadata": {},
   "source": [
    "And, for small PGMs, you can afford to build a table view of the joint distribution. The helper function below, from pgmini, builds a TabularFactor out of the product of all factors in the PGM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fbe80e14-80c3-43ab-85a7-f488ee74d460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    B    C    D         Value\n",
       "---  ---  ---  ---  ----------\n",
       "a0   b0   c0   d0   300000\n",
       "a0   b0   c0   d1   300000\n",
       "a0   b0   c1   d0   300000\n",
       "a0   b0   c1   d1       30\n",
       "a0   b1   c0   d0      500\n",
       "a0   b1   c0   d1      500\n",
       "a0   b1   c1   d0        5e+06\n",
       "a0   b1   c1   d1      500\n",
       "a1   b0   c0   d0      100\n",
       "a1   b0   c0   d1        1e+06\n",
       "a1   b0   c1   d0      100\n",
       "a1   b0   c1   d1      100\n",
       "a1   b1   c0   d0       10\n",
       "a1   b1   c0   d1   100000\n",
       "a1   b1   c1   d0   100000\n",
       "a1   b1   c1   d1   100000"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgm_to_tabular_factor(misconception_mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f9819",
   "metadata": {},
   "source": [
    "# Medical BN #\n",
    "\n",
    "This is the PGM that will be used for the exercises. It should be familiar to you, since we used it last week.\n",
    "\n",
    "When going to the doctor, we dont expect the doctor to say; you have a fever and bronchitis with this probability, a fever and no bronchitis with this probability, no fever but bronchitis with this probability and no fever nor bronchitis with this probability.\n",
    "\n",
    "A normal doctor would just say; it is likely that you have/don't have fever and that you have/don't have bronchites. In the first part of the notebook we will look at how to efficiently find this mode/most likely outcome.\n",
    "\n",
    "Similar to last week, we define the medical BN example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "54c31ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_medical_example():\n",
    "    medical_outcome_spaces = {\n",
    "        'I': OutcomeSpace(['i0', 'i1']),\n",
    "        'S': OutcomeSpace(['s0', 's1']),\n",
    "        'ST': OutcomeSpace(['st0', 'st1']),\n",
    "        'F': OutcomeSpace(['f0', 'f1']),\n",
    "        'B': OutcomeSpace(['b0', 'b1']),\n",
    "        'C': OutcomeSpace(['c0', 'c1']),\n",
    "        'W': OutcomeSpace(['w0', 'w1'])\n",
    "    }\n",
    "    \n",
    "    # Factors\n",
    "    phi_I = TabularCPDFactor([], 'I', medical_outcome_spaces, [0.9, 0.1])\n",
    "    phi_S = TabularCPDFactor([], 'S', medical_outcome_spaces, [0.7, 0.3])\n",
    "    phi_ST = TabularCPDFactor(['I'], 'ST', medical_outcome_spaces, [[0.99, 0.01], [0.8, 0.2]])\n",
    "    phi_F = TabularCPDFactor(['I'], 'F', medical_outcome_spaces, [[0.9, 0.1], [0.05, 0.95]])\n",
    "    phi_B = TabularCPDFactor(['I', 'S'], 'B', medical_outcome_spaces, \n",
    "    [[[0.999, 0.001], [0.25, 0.75]], [[0.05, 0.95], [0.1, 0.9]]])\n",
    "    phi_C = TabularCPDFactor(['B'], 'C', medical_outcome_spaces, [[0.95, 0.05], [0.25, 0.75]])\n",
    "    phi_W = TabularCPDFactor(['B'], 'W', medical_outcome_spaces, [[0.999, 0.001], [0.5, 0.5]])\n",
    "    \n",
    "    return BayesianNetwork([phi_I, phi_S, phi_ST, phi_F, phi_B, phi_C, phi_W])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7a200-3339-443e-9a4d-75709b3cfcf8",
   "metadata": {},
   "source": [
    "You can view the BN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0f966580-b08a-4d01-b2a4-3abd54ed3682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "| parents   | child   |\n",
      "+===========+=========+\n",
      "|           | I       |\n",
      "+-----------+---------+\n",
      "|           | S       |\n",
      "+-----------+---------+\n",
      "| I         | F       |\n",
      "+-----------+---------+\n",
      "| I         | ST      |\n",
      "+-----------+---------+\n",
      "| I, S      | B       |\n",
      "+-----------+---------+\n",
      "| B         | W       |\n",
      "+-----------+---------+\n",
      "| B         | C       |\n",
      "+-----------+---------+\n"
     ]
    }
   ],
   "source": [
    "medical_bn = make_medical_example()\n",
    "print(medical_bn.dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc2f3a-50a7-4520-9ff9-e681a24a317b",
   "metadata": {},
   "source": [
    "And, because it's a small PGM, we can construct and display the tabular view of the joint distribution. But mind that this is just to help us understand what we are doing (in practice, you can almost never do this, and that's why we create algorithms to make it unnecessary to ever build this thing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ae52069c-6cf7-4f2d-9749-50ab15f61b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I    S    ST    F    B    C    W          Value\n",
      "---  ---  ----  ---  ---  ---  ---  -----------\n",
      "i0   s0   st0   f0   b0   c0   w0   0.532198\n",
      "i0   s0   st0   f0   b0   c0   w1   0.00053273\n",
      "i0   s0   st0   f0   b0   c1   w0   0.0280104\n",
      "i0   s0   st0   f0   b0   c1   w1   2.80384e-05\n",
      "i0   s0   st0   f0   b1   c0   w0   7.01662e-05\n",
      "i0   s0   st0   f0   b1   c0   w1   7.01662e-05\n",
      "i0   s0   st0   f0   b1   c1   w0   0.000210499\n",
      "i0   s0   st0   f0   b1   c1   w1   0.000210499\n",
      "i0   s0   st0   f1   b0   c0   w0   0.0591331\n",
      "i0   s0   st0   f1   b0   c0   w1   5.91922e-05\n",
      "i0   s0   st0   f1   b0   c1   w0   0.00311227\n",
      "i0   s0   st0   f1   b0   c1   w1   3.11538e-06\n",
      "i0   s0   st0   f1   b1   c0   w0   7.79625e-06\n",
      "i0   s0   st0   f1   b1   c0   w1   7.79625e-06\n",
      "i0   s0   st0   f1   b1   c1   w0   2.33888e-05\n",
      "i0   s0   st0   f1   b1   c1   w1   2.33888e-05\n",
      "i0   s0   st1   f0   b0   c0   w0   0.00537573\n",
      "i0   s0   st1   f0   b0   c0   w1   5.38111e-06\n",
      "i0   s0   st1   f0   b0   c1   w0   0.000282933\n",
      "i0   s0   st1   f0   b0   c1   w1   2.83217e-07\n",
      "i0   s0   st1   f0   b1   c0   w0   7.0875e-07\n",
      "i0   s0   st1   f0   b1   c0   w1   7.0875e-07\n",
      "i0   s0   st1   f0   b1   c1   w0   2.12625e-06\n",
      "i0   s0   st1   f0   b1   c1   w1   2.12625e-06\n",
      "i0   s0   st1   f1   b0   c0   w0   0.000597304\n",
      "i0   s0   st1   f1   b0   c0   w1   5.97901e-07\n",
      "i0   s0   st1   f1   b0   c1   w0   3.1437e-05\n",
      "i0   s0   st1   f1   b0   c1   w1   3.14685e-08\n",
      "i0   s0   st1   f1   b1   c0   w0   7.875e-08\n",
      "i0   s0   st1   f1   b1   c0   w1   7.875e-08\n",
      "i0   s0   st1   f1   b1   c1   w0   2.3625e-07\n",
      "i0   s0   st1   f1   b1   c1   w1   2.3625e-07\n",
      "i0   s1   st0   f0   b0   c0   w0   0.0570782\n",
      "i0   s1   st0   f0   b0   c0   w1   5.71354e-05\n",
      "i0   s1   st0   f0   b0   c1   w0   0.00300412\n",
      "i0   s1   st0   f0   b0   c1   w1   3.00713e-06\n",
      "i0   s1   st0   f0   b1   c0   w0   0.0225534\n",
      "i0   s1   st0   f0   b1   c0   w1   0.0225534\n",
      "i0   s1   st0   f0   b1   c1   w0   0.0676603\n",
      "i0   s1   st0   f0   b1   c1   w1   0.0676603\n",
      "i0   s1   st0   f1   b0   c0   w0   0.00634203\n",
      "i0   s1   st0   f1   b0   c0   w1   6.34838e-06\n",
      "i0   s1   st0   f1   b0   c1   w0   0.000333791\n",
      "i0   s1   st0   f1   b0   c1   w1   3.34125e-07\n",
      "i0   s1   st0   f1   b1   c0   w0   0.00250594\n",
      "i0   s1   st0   f1   b1   c0   w1   0.00250594\n",
      "i0   s1   st0   f1   b1   c1   w0   0.00751781\n",
      "i0   s1   st0   f1   b1   c1   w1   0.00751781\n",
      "i0   s1   st1   f0   b0   c0   w0   0.000576548\n",
      "i0   s1   st1   f0   b0   c0   w1   5.77125e-07\n",
      "i0   s1   st1   f0   b0   c1   w0   3.03446e-05\n",
      "i0   s1   st1   f0   b0   c1   w1   3.0375e-08\n",
      "i0   s1   st1   f0   b1   c0   w0   0.000227813\n",
      "i0   s1   st1   f0   b1   c0   w1   0.000227813\n",
      "i0   s1   st1   f0   b1   c1   w0   0.000683438\n",
      "i0   s1   st1   f0   b1   c1   w1   0.000683438\n",
      "i0   s1   st1   f1   b0   c0   w0   6.40609e-05\n",
      "i0   s1   st1   f1   b0   c0   w1   6.4125e-08\n",
      "i0   s1   st1   f1   b0   c1   w0   3.37163e-06\n",
      "i0   s1   st1   f1   b0   c1   w1   3.375e-09\n",
      "i0   s1   st1   f1   b1   c0   w0   2.53125e-05\n",
      "i0   s1   st1   f1   b1   c0   w1   2.53125e-05\n",
      "i0   s1   st1   f1   b1   c1   w0   7.59375e-05\n",
      "i0   s1   st1   f1   b1   c1   w1   7.59375e-05\n",
      "i1   s0   st0   f0   b0   c0   w0   0.000132867\n",
      "i1   s0   st0   f0   b0   c0   w1   1.33e-07\n",
      "i1   s0   st0   f0   b0   c1   w0   6.993e-06\n",
      "i1   s0   st0   f0   b0   c1   w1   7e-09\n",
      "i1   s0   st0   f0   b1   c0   w0   0.0003325\n",
      "i1   s0   st0   f0   b1   c0   w1   0.0003325\n",
      "i1   s0   st0   f0   b1   c1   w0   0.0009975\n",
      "i1   s0   st0   f0   b1   c1   w1   0.0009975\n",
      "i1   s0   st0   f1   b0   c0   w0   0.00252447\n",
      "i1   s0   st0   f1   b0   c0   w1   2.527e-06\n",
      "i1   s0   st0   f1   b0   c1   w0   0.000132867\n",
      "i1   s0   st0   f1   b0   c1   w1   1.33e-07\n",
      "i1   s0   st0   f1   b1   c0   w0   0.0063175\n",
      "i1   s0   st0   f1   b1   c0   w1   0.0063175\n",
      "i1   s0   st0   f1   b1   c1   w0   0.0189525\n",
      "i1   s0   st0   f1   b1   c1   w1   0.0189525\n",
      "i1   s0   st1   f0   b0   c0   w0   3.32168e-05\n",
      "i1   s0   st1   f0   b0   c0   w1   3.325e-08\n",
      "i1   s0   st1   f0   b0   c1   w0   1.74825e-06\n",
      "i1   s0   st1   f0   b0   c1   w1   1.75e-09\n",
      "i1   s0   st1   f0   b1   c0   w0   8.3125e-05\n",
      "i1   s0   st1   f0   b1   c0   w1   8.3125e-05\n",
      "i1   s0   st1   f0   b1   c1   w0   0.000249375\n",
      "i1   s0   st1   f0   b1   c1   w1   0.000249375\n",
      "i1   s0   st1   f1   b0   c0   w0   0.000631118\n",
      "i1   s0   st1   f1   b0   c0   w1   6.3175e-07\n",
      "i1   s0   st1   f1   b0   c1   w0   3.32167e-05\n",
      "i1   s0   st1   f1   b0   c1   w1   3.325e-08\n",
      "i1   s0   st1   f1   b1   c0   w0   0.00157937\n",
      "i1   s0   st1   f1   b1   c0   w1   0.00157937\n",
      "i1   s0   st1   f1   b1   c1   w0   0.00473812\n",
      "i1   s0   st1   f1   b1   c1   w1   0.00473812\n",
      "i1   s1   st0   f0   b0   c0   w0   0.000113886\n",
      "i1   s1   st0   f0   b0   c0   w1   1.14e-07\n",
      "i1   s1   st0   f0   b0   c1   w0   5.994e-06\n",
      "i1   s1   st0   f0   b0   c1   w1   6e-09\n",
      "i1   s1   st0   f0   b1   c0   w0   0.000135\n",
      "i1   s1   st0   f0   b1   c0   w1   0.000135\n",
      "i1   s1   st0   f0   b1   c1   w0   0.000405\n",
      "i1   s1   st0   f0   b1   c1   w1   0.000405\n",
      "i1   s1   st0   f1   b0   c0   w0   0.00216383\n",
      "i1   s1   st0   f1   b0   c0   w1   2.166e-06\n",
      "i1   s1   st0   f1   b0   c1   w0   0.000113886\n",
      "i1   s1   st0   f1   b0   c1   w1   1.14e-07\n",
      "i1   s1   st0   f1   b1   c0   w0   0.002565\n",
      "i1   s1   st0   f1   b1   c0   w1   0.002565\n",
      "i1   s1   st0   f1   b1   c1   w0   0.007695\n",
      "i1   s1   st0   f1   b1   c1   w1   0.007695\n",
      "i1   s1   st1   f0   b0   c0   w0   2.84715e-05\n",
      "i1   s1   st1   f0   b0   c0   w1   2.85e-08\n",
      "i1   s1   st1   f0   b0   c1   w0   1.4985e-06\n",
      "i1   s1   st1   f0   b0   c1   w1   1.5e-09\n",
      "i1   s1   st1   f0   b1   c0   w0   3.375e-05\n",
      "i1   s1   st1   f0   b1   c0   w1   3.375e-05\n",
      "i1   s1   st1   f0   b1   c1   w0   0.00010125\n",
      "i1   s1   st1   f0   b1   c1   w1   0.00010125\n",
      "i1   s1   st1   f1   b0   c0   w0   0.000540959\n",
      "i1   s1   st1   f1   b0   c0   w1   5.415e-07\n",
      "i1   s1   st1   f1   b0   c1   w0   2.84715e-05\n",
      "i1   s1   st1   f1   b0   c1   w1   2.85e-08\n",
      "i1   s1   st1   f1   b1   c0   w0   0.00064125\n",
      "i1   s1   st1   f1   b1   c0   w1   0.00064125\n",
      "i1   s1   st1   f1   b1   c1   w0   0.00192375\n",
      "i1   s1   st1   f1   b1   c1   w1   0.00192375\n"
     ]
    }
   ],
   "source": [
    "medical_bn_table = pgm_to_tabular_factor(medical_bn) # this can only be done for toy examples, as a table view of the joint distribution is typically intractable\n",
    "print(medical_bn_table) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3288e64",
   "metadata": {},
   "source": [
    "# Max-Product (or MAP) Inference\n",
    "\n",
    "The so-called \"MAP inference\" task (MAP stands for maximum a posteriori) is the task of finding the **mode** of the distribution. The _mode_ is the joint assignment that the model assigns highest probability to. \n",
    "\n",
    "That is, for a distribution $P_\\Phi(\\boldsymbol{X})$ the mode is the outcome $\\boldsymbol{x}^\\star$ below:\n",
    "\n",
    "$\\boldsymbol{x}^\\star = \\arg\\!\\max_{\\boldsymbol{x} \\in \\text{Val}(\\boldsymbol{X})}~P_\\Phi(\\boldsymbol{X}=\\boldsymbol{x})= \\arg\\!\\max_{\\boldsymbol{x} \\in \\text{Val}(\\boldsymbol{X})}~\\tilde P_\\Phi(\\boldsymbol{X}=\\boldsymbol{x})$\n",
    "\n",
    "The MAP inference task is more generally known as `maximum-product` inference since we are computing the maximum of a large _product of factors_.\n",
    "\n",
    "It is worthwhile to realise that max-product inference does not require a normalised distribution, since the argmax (mode) is the same whether or not the factor product is normalised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e910c-2f45-4270-a9c9-79ad48e1607c",
   "metadata": {},
   "source": [
    "## Naive Max-Product Inference\n",
    "\n",
    "A naive implementation of max-product inference is essentially an exhaustive search: 1) enumerate joint outcomes, 2) assess their (unnormalised) probability under the model (e.g., using `PGM.evaluate`), 3) find the outcome whose (unnormalised) probability is maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe8dbc5-196e-4184-a742-932fb85dce04",
   "metadata": {},
   "source": [
    "**EXERCISE - Max-Product Inference - Exhaustive** Find and report the mode of the `medical_bn` via an exhaustive procedure.\n",
    "\n",
    "To know if your solution is correct, you can compare your result to what `TabularFactor.argmax()` produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "210f4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustive_max_product_inference(pgm: PGM):  # other implementations are possible\n",
    "    \"\"\"Return the argmax assignment (dict) under the PGM\"\"\"\n",
    "    \n",
    "    variables = list(pgm.outcome_spaces.keys())\n",
    "    domains = [pgm.outcome_spaces[v].outcomes for v in variables]\n",
    "\n",
    "    best_assignment = None\n",
    "    best_score = -math.inf\n",
    "\n",
    "    for values in itertools.product(*domains):\n",
    "        assignment = dict(zip(variables, values))\n",
    "\n",
    "        score = pgm.evaluate(assignment)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_assignment = assignment\n",
    "\n",
    "    return best_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e94f1-078f-4ec2-9ee1-d56e7e6d5e5a",
   "metadata": {},
   "source": [
    "Example of how to test it (using the Misconception example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d55bf5d4-7090-4f9c-a8bd-15431ee9fc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exhaustive search for Misconception example\n",
      "- argmax {'A': 'a0', 'B': 'b1', 'C': 'c1', 'D': 'd0'}\n",
      "- argmax {'A': 'a0', 'B': 'b1', 'C': 'c1', 'D': 'd0'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Exhaustive search for Misconception example\")\n",
    "print(\"- argmax\", exhaustive_max_product_inference(misconception_mn))\n",
    "print(\"- argmax\", pgm_to_tabular_factor(misconception_mn).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df75f82-6412-47ef-8c44-d668be4ee2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03448415",
   "metadata": {},
   "source": [
    "## Max-Product for Conditional Queries ##\n",
    "\n",
    "Suppose we have some evidence $\\boldsymbol{E}=\\boldsymbol{e}$ and we want to find the maximum over some (but not necessarily) all of the variables that remain. \n",
    "\n",
    "We can do that by first expressing the appropriate conditional query using the VE algorithm as we did last week. We have provided an implementation for you. To distinguish VE when used for `sum-product` (marginal inference) as opposed to `max-product` (MAP inference), we have called it `sum_product_variable_elimination` in pgmini.\n",
    "\n",
    "Remark: because the rest of this notebook will concern efficient inference, the function `sum_product_variable_elimination` does not return the product of the factors that remain in VE, instead it returns a Markov Network that uses those factors to represent the final query. This is the standard way to interact with VE in most professional PGM libraries. \n",
    "\n",
    "After the appropriate query has been expressed as a Markov network (which is the return of `sum_product_variable_elimination`) we can use it for max-product inference. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc92ba-68fd-486e-98f2-f41a433e371f",
   "metadata": {},
   "source": [
    "Here we demo the `sum_product_variable_elimination` function for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "382909ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the min degree heuristic to efficiently run VE\n",
    "def min_degree(node, pgm: PGM):\n",
    "    \"\"\"Return number of factors whose scope contains the rv\"\"\"\n",
    "    return (sum(1 if node in factor else 0 for factor in pgm.iterfactors()), str(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "abb8f5e7-aa7c-4e74-a76f-c68fb96e1a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(B, D|C=c1) MN:\n",
      "+-------------------+\n",
      "| nodes and edges   |\n",
      "+===================+\n",
      "| B                 |\n",
      "+-------------------+\n",
      "| D                 |\n",
      "+-------------------+\n",
      "| B -- D            |\n",
      "+-------------------+\n",
      "Convert MN to table\n",
      "B    D           Value\n",
      "---  ---  ------------\n",
      "b0   d0   300100\n",
      "b0   d1      130\n",
      "b1   d0        5.1e+06\n",
      "b1   d1   100500\n",
      "Exhaustive argmax\n",
      "{'B': 'b1', 'D': 'd0'}\n"
     ]
    }
   ],
   "source": [
    "misconception_mn = make_misconception_example()\n",
    "misconception_mn_BD_given_c1 = sum_product_variable_elimination(\n",
    "    misconception_mn,\n",
    "    {'B', 'D'},\n",
    "    {'C': 'c1'},\n",
    "    key=functools.partial(min_degree, pgm=misconception_mn),\n",
    ")\n",
    "print(\"P(B, D|C=c1) MN:\")\n",
    "print(misconception_mn_BD_given_c1.graph)\n",
    "print(\"Convert MN to table\")\n",
    "print(pgm_to_tabular_factor(misconception_mn_BD_given_c1))\n",
    "print(\"Exhaustive argmax\")\n",
    "print(exhaustive_max_product_inference(misconception_mn_BD_given_c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897c94f-993d-4d86-8abf-47aff0596ed3",
   "metadata": {},
   "source": [
    "**EXERCISE - Max-Product Inference - Exhaustive - Conditional Query**. For the `medical_bn`, what is the most likely outcome of fever and bronchitis and coughing given that we know someone has influenza. For this exercise you should use `sum_product_variable_elimination` to obtain an MN over the correct distribution, and then you can use an exhaustive search approach to reporting the argmax.\n",
    "\n",
    "For VE, you can use the `min_degree` order heuristic implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "47647de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(F, B, C | I=i1) as MN:\n",
      "+-------------------+\n",
      "| nodes and edges   |\n",
      "+===================+\n",
      "| B                 |\n",
      "+-------------------+\n",
      "| C                 |\n",
      "+-------------------+\n",
      "| F                 |\n",
      "+-------------------+\n",
      "| B -- C            |\n",
      "+-------------------+\n",
      "\n",
      "As table:\n",
      "F    B    C         Value\n",
      "---  ---  ---  ----------\n",
      "f0   b0   c0   0.00030875\n",
      "f0   b0   c1   1.625e-05\n",
      "f0   b1   c0   0.00116875\n",
      "f0   b1   c1   0.00350625\n",
      "f1   b0   c0   0.00586625\n",
      "f1   b0   c1   0.00030875\n",
      "f1   b1   c0   0.0222063\n",
      "f1   b1   c1   0.0666188\n",
      "\n",
      "Exhaustive MAP for (F, B, C | I=i1):\n",
      "{'F': 'f1', 'B': 'b1', 'C': 'c1'}\n"
     ]
    }
   ],
   "source": [
    "medical_mn_FBC_given_I1 = sum_product_variable_elimination(\n",
    "    medical_bn,\n",
    "    {'F', 'B', 'C'},\n",
    "    {'I': 'i1'},\n",
    "    key=functools.partial(min_degree, pgm=medical_bn),\n",
    ")\n",
    "\n",
    "print(\"P(F, B, C | I=i1) as MN:\")\n",
    "print(medical_mn_FBC_given_I1.graph)\n",
    "\n",
    "print(\"\\nAs table:\")\n",
    "print(pgm_to_tabular_factor(medical_mn_FBC_given_I1))\n",
    "\n",
    "# 2) Max-product (exhaustive) op deze MN\n",
    "print(\"\\nExhaustive MAP for (F, B, C | I=i1):\")\n",
    "print(exhaustive_max_product_inference(medical_mn_FBC_given_I1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd1dcc-0996-41c7-b2f2-67c1dfd936de",
   "metadata": {},
   "source": [
    "# Max-Product VE (or VE for MAP Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d469a-d7ae-421d-bada-aa59a06b573a",
   "metadata": {},
   "source": [
    "Max-product VE is a relatively simple modification of sum-product VE where instead of _marginalising_ a variable out of a factor, when we eliminate it, we _maximise_ it out of the factor. \n",
    "\n",
    "That is, if we have a node $X$ and $\\phi_1(X, Y)$ and $\\phi_2(X, Z)$ are the relevant factors (whose scopes contain $X$), then sum-product VE eliminates $X$ via $\\tau(Y, Z) = \\sum_{x \\in \\text{Val}(X)} \\phi_1(X=x, Y)\\phi_2(X=x, Z)$, while max-product VE eliminates $X$ via $\\tau(Y, Z) = \\max_{x \\in \\text{Val}(X)} \\phi_1(X=x, Y)\\phi_2(X=x, Z)$.\n",
    "\n",
    "By keeping track of the intermediate factors, which max-product VE maximises over, we can construct the assignment of the rvs that correspond to $\\arg\\!\\max_{X,Y,Z} \\tilde P_\\Phi(X, Y, Z)$.\n",
    "\n",
    "\n",
    "We implemented the function `max_product_variable_elimination` for you in pgmini. Feel free to inspect the function on github if you want to better understand the differences compared to sum-product VE. The function returns 2 things: the mode and a Markov network with the mode (mostly you can ignore this, you only really need the mode). See the demo below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b356841b-7d61-4a92-8d94-4e4c8ea8dc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VE for the Misconception example\n",
      "- argmax {'D': 'd0', 'C': 'c1', 'B': 'b1', 'A': 'a0'}\n",
      "- unnormalised probability 5000000.0\n"
     ]
    }
   ],
   "source": [
    "print(\"VE for the Misconception example\")\n",
    "trace = []\n",
    "misconception_argmax, _ = max_product_variable_elimination(\n",
    "    misconception_mn, \n",
    "    key=functools.partial(min_degree, pgm=misconception_mn), \n",
    "    trace=trace # if you provide a `trace` list, you can then see the scope of the intermediate factor at each elimination step\n",
    ")\n",
    "# Note this operation still requires the whole table\n",
    "# You will implement an efficient method later\n",
    "print(\"- argmax\", misconception_argmax)\n",
    "print(\"- unnormalised probability\", misconception_mn.evaluate(misconception_argmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9b83b-cec0-4fc1-8bc3-0cb027b81f6a",
   "metadata": {},
   "source": [
    "**EXERCISE - Max-Product VE** \n",
    "\n",
    "For the `medical_bn`, report:\n",
    "\n",
    "1. The mode of the joint distribution (without evidence) and its **normalised** probability. \n",
    "2. The mode of the conditional distribution $P(B, C|I=i^1)$ and its **normalised** probability.\n",
    "\n",
    "Your implementation should **never** build the tabular view of these distributions, instead it should use VE for sum-product inference and for max-product inference and, of course, for obtaining normalisation constants if they are necessary.\n",
    "\n",
    "Compare the complexity to the exhaustive case in terms of the size of the largest tabular factor built.\n",
    "\n",
    "Tip: if you are not sure you have normalised correctly, experiment with a small network for which you know the normaliser (e.g., the Misconception MN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "abb746f1-e5ee-4b39-bda1-afd3d24ed42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint mode P(I,S,ST,F,B,C,W):\n",
      "  argmax = {'I': 'i0', 'B': 'b0', 'S': 's0', 'W': 'w0', 'ST': 'st0', 'F': 'f0', 'C': 'c0'}\n",
      "  normalised probability = 0.5321975062634999\n",
      "\n",
      "Mode of P(B, C | I = i1):\n",
      "  argmax = {'B': 'b1', 'C': 'c1'}\n",
      "  unnormalised probability = 0.070125\n",
      "  normalised probability   = 0.70125\n"
     ]
    }
   ],
   "source": [
    "trace = []\n",
    "joint_mode, _ = max_product_variable_elimination(\n",
    "    medical_bn,\n",
    "    key=functools.partial(min_degree, pgm=medical_bn),\n",
    "    trace=trace,\n",
    ")\n",
    "\n",
    "joint_prob = medical_bn.evaluate(joint_mode)   # = P(mode)\n",
    "\n",
    "print(\"Joint mode P(I,S,ST,F,B,C,W):\")\n",
    "print(\"  argmax =\", joint_mode)\n",
    "print(\"  normalised probability =\", joint_prob)   # normaliser = 1 voor een BN\n",
    "\n",
    "medical_mn_BC_given_i1 = sum_product_variable_elimination(\n",
    "    medical_bn,\n",
    "    {'B', 'C'},             # query-variabelen\n",
    "    {'I': 'i1'},            # evidence\n",
    "    key=functools.partial(min_degree, pgm=medical_bn),\n",
    ")\n",
    "\n",
    "trace = []\n",
    "bc_mode, _ = max_product_variable_elimination(\n",
    "    medical_mn_BC_given_i1,\n",
    "    key=functools.partial(min_degree, pgm=medical_mn_BC_given_i1),\n",
    "    trace=trace,\n",
    ")\n",
    "\n",
    "bc_unnorm = medical_mn_BC_given_i1.evaluate(bc_mode)\n",
    "\n",
    "B_vals = medical_bn.outcome_spaces['B'].outcomes\n",
    "C_vals = medical_bn.outcome_spaces['C'].outcomes\n",
    "\n",
    "Z_bc = 0.0\n",
    "for b in B_vals:\n",
    "    for c in C_vals:\n",
    "        Z_bc += medical_mn_BC_given_i1.evaluate({'B': b, 'C': c})\n",
    "\n",
    "bc_norm = bc_unnorm / Z_bc\n",
    "\n",
    "print(\"\\nMode of P(B, C | I = i1):\")\n",
    "print(\"  argmax =\", bc_mode)\n",
    "print(\"  unnormalised probability =\", bc_unnorm)\n",
    "print(\"  normalised probability   =\", bc_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b985965-891c-4f78-97a3-b0e3f9a78f57",
   "metadata": {},
   "source": [
    "# Sampling Exactly but Intractably\n",
    "\n",
    "In the first part of this notebook we focused on getting the assignment with the most likely outcome (the mode). \n",
    "\n",
    "In the second part of this notebook we focus on getting assignments randomly, where the probability of getting an assignment is defined by the probability of that assignment _according to the joint distribution_ (i.e. sampling). \n",
    "\n",
    "The mode is just the assignment that has the highest probability of being sampled.\n",
    "\n",
    "\n",
    "Sampling \"exactly\" means sampling from the correct distribution. For a general PGM, there is no tractable algorithm to sample exactly. There are some special cases (BNs without evidence, certain BNs and certain MNs even with evidence), but we are covering the most general case. \n",
    "\n",
    "For small PGMs, we can still demonstrate what it means to sample exactly by essentially creating the table view of the normalised joint distribution and then sampling from it via the icdf method.\n",
    "\n",
    "This is what we demonstrate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cf3cb3dc-0698-4a39-95b5-e5e33aef4237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_exactly_but_intractably(pgm: PGM, size=None, rng=np.random.default_rng()):\n",
    "    \"\"\"\n",
    "    This builds a large table view of the PGM and then draws samples using the exact icdf method.\n",
    "    pgm: the PGM we are sampling from\n",
    "    size: as in numpy\n",
    "        - if size is None, we return one a sample (a dict)\n",
    "        - if size is a number, we return a list of samples (each sample is a dict)\n",
    "    rng: numpy random number generator\n",
    "    \"\"\"\n",
    "    # build table (this is generally intractable)\n",
    "    table_view = pgm_to_tabular_factor(pgm).normalize()\n",
    "    # icdf sample\n",
    "    return table_view.sample(size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6b668bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 'a0', 'B': 'b0', 'C': 'c0', 'D': 'd0'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_exactly_but_intractably(misconception_mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a0f00-d5e4-4d8e-bc1d-b78a8c776ded",
   "metadata": {},
   "source": [
    "And, as we are sampling from a full table view anyway, we can also have conditional queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "49c8a1ac-2635-4e94-800d-07cebf158a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': 'b1', 'D': 'd0'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_exactly_but_intractably(misconception_mn_BD_given_c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da5ff9-2ee4-4a1f-bc36-9152380c61ab",
   "metadata": {},
   "source": [
    "`pgmini.util` has some rather useful helper functions:\n",
    "- `make_samples_df` makes an empirical distribution out of a list of samples\n",
    "- `df_to_factor` can convert a data frame (such as obtained by `make_samples_df` to a TabulaFactor)\n",
    "- `tvd` can compare distribution and/or empirical estimates of distributions in terms of Total Variation Distance\n",
    "\n",
    "[Here](https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures) you can check the explanation of [Total Variation Distance](https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures)\n",
    "\n",
    "We will be using these below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c96c50-e4f8-468a-86ee-0ee2c7b4b9dd",
   "metadata": {},
   "source": [
    "Below we demo how you can use these helper functions to compare the joint distribution to an empirical distribution obtained by sampling in terms of TVD. Of course, this only works when we are able to sample exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a23f2e20-00c9-4480-8e4e-1bf3805e23db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misconception TVD(PGM->df, samples->df): 0.072802617108961\n"
     ]
    }
   ],
   "source": [
    "# draw samples exactly but intractably, \n",
    "# convert the samples to a DataFrame\n",
    "# convert the entire PGM to a DataFrame and compare to the samples DataFrame via TVD\n",
    "misconception_exact_samples_df = make_samples_df(sample_exactly_but_intractably(misconception_mn, size=100))\n",
    "print(\"Misconception TVD(PGM->df, samples->df):\", tvd(pgm_to_df(misconception_mn, normalize=True), misconception_exact_samples_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4067d999-bf9f-4fe0-9bdf-3a6640ad4d05",
   "metadata": {},
   "source": [
    "**EXERCISE - Exact but Intractable Sampling**\n",
    "\n",
    "For the medical BN, sample exactly and compare the empirical distribution to the target distribution in terms of TVD in the following cases:\n",
    "1. No evidence\n",
    "2. Evidence: $S=s^1$\n",
    "\n",
    "For each case above, you should experiment with sample size (10, 100 and 1000 samples). For each experiment (1 and 2) and for each sample size (10, 100, 1000) you should repeat the experiment 20 times so that you can see how TVD varies. Plot the results conveniently (for example, using a boxplot). \n",
    "\n",
    "Finally, discuss what you observe in your experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3b913eb7-d655-4276-ad98-c81889a0f085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>size</th>\n",
       "      <th>rep</th>\n",
       "      <th>TVD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_evidence</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.388199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_evidence</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.394383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_evidence</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.382137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_evidence</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.329038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no_evidence</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.527583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          case  size  rep       TVD\n",
       "0  no_evidence    10    0  0.388199\n",
       "1  no_evidence    10    1  0.394383\n",
       "2  no_evidence    10    2  0.382137\n",
       "3  no_evidence    10    3  0.329038\n",
       "4  no_evidence    10    4  0.527583"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_tvd_experiments(pgm, label, sizes=(10, 100, 1000), repeats=20, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for size in sizes:\n",
    "        for rep in range(repeats):\n",
    "            samples = sample_exactly_but_intractably(pgm, size=size, rng=rng)\n",
    "            samples_df = make_samples_df(samples)\n",
    "\n",
    "            target_df = pgm_to_df(pgm, normalize=True)\n",
    "\n",
    "            tvd_val = tvd(target_df, samples_df)\n",
    "\n",
    "            records.append({\n",
    "                \"case\": label,\n",
    "                \"size\": size,\n",
    "                \"rep\": rep,\n",
    "                \"TVD\": tvd_val,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "tvd_no_evidence_df = run_tvd_experiments(medical_bn, label=\"no_evidence\")\n",
    "\n",
    "all_vars = set(medical_bn.outcome_spaces.keys())\n",
    "query_vars = all_vars - {\"S\"}\n",
    "\n",
    "medical_mn_given_S1 = sum_product_variable_elimination(\n",
    "    medical_bn,\n",
    "    query_vars,\n",
    "    {\"S\": \"s1\"},\n",
    "    key=functools.partial(min_degree, pgm=medical_bn),\n",
    ")\n",
    "\n",
    "tvd_S1_df = run_tvd_experiments(medical_mn_given_S1, label=\"S=s1\")\n",
    "\n",
    "tvd_results_df = pd.concat([tvd_no_evidence_df, tvd_S1_df], ignore_index=True)\n",
    "tvd_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "affcf914-e532-40e3-a0b3-2563e816a9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAF3CAYAAAC41ignAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAASHJJREFUeJzt3Ql8VNXZ+PEn+waRJUIwrIKyCAYJEhEBkU0EBRWKaAWppn1V6sJSgfctiFqoLCnWoigtbkWgIqIvKEJZBBREggsopAUDKBB2CFkIWeb/ec77n2lCJmGS3JuZSX7fz+d6vXfu3Dn35jBnnnu2AIfD4RAAAAAAAGC5QOtPCQAAAAAACLoBAAAAALARNd0AAAAAANiEoBsAAAAAAJsQdAMAAAAAYBOCbgAAAAAAbELQDQAAAACATQi6AQAAAACwCUE3AAAAAAA2IegGAAAA4LGHHnpImjdvftnjDhw4IAEBAfLmm29yd1GjEXQDfmT//v3ym9/8Rq6++moJDw+X6Oho6datm7z00kuSk5Mj/qKwsFDefvttSUxMlHr16knt2rXl2muvlZEjR8q2bduqJA2pqany9NNPy80332zupf4o0B8HAADfVF3KQAA1T7C3EwDAM6tWrZJhw4ZJWFiYCU7bt28vFy9elC1btsiECRPk+++/l9dff90vbucTTzwh8+bNk8GDB8sDDzwgwcHBJgj+5JNPzI+pm266yfY0bN26Vf785z9Lu3btpG3btvLNN9/Y/pkAgIqpTmVgdbBgwQLzAB2AZwi6AT+QlpYm9913nzRr1kzWr18vjRo1cr32+OOPy759+8wPEn9w7NgxeeWVVyQpKanED6S5c+fKiRMnqiQdd911l5w9e9bUss+ePZugGwB8VHUqA6uLkJAQbycB8Cs0Lwf8wMyZMyUzM1P+9re/Ffux4dSqVSt58sknXdtvvPGG3HbbbdKgQQNTK6C1ua+++mqJ9+3YsUP69+8vMTExEhERIS1atJBf/epXxY7RJ9kaDF933XWmOV/Dhg1N874zZ84UO+7cuXOyd+9es77cjyeHw2GaBF5Km3hrmivr/Pnz8tRTT5n+Znr9es6+ffvKzp07Xcc4m7UDAHxbdSoDlZZNgwYNMrX0Xbp0MefVVl7a7epSP/74o6nh1zIrMjLStASr6AMGbU3WvXt3iYqKMuXfwIEDTQsBJ30AreXwwYMHS7x30qRJEhoa6rpud3269UG27r/iiiukTp06MmrUKLPPHb1XQ4cONdel19+5c2f56KOPih2j/cA1PZ9//rmMHTtWrrzySpP2u+++2+0Der2+nj17mmvTrgc33nijvPvuu8WO+fLLL+X22283adT7qcfr+QG7EXQDfuB///d/TYGs/Y89oT8utEZg8uTJMmfOHGnSpIk89thjpkm30/Hjx6Vfv36mH/PEiRPl5ZdfNk29L+1TrT8utOmes9/c6NGjZdGiReaHSl5enuu4Dz74wDTT1nVZNF3qvffek+zs7DKP1R87J0+e9Ggpmpb/+q//Mvfg3nvvNbXq48ePNz+o9uzZ49H9AwD4jupUBjpp7bwGnfpAWNNYt25dE7AWDYK1ZZhe86effmrS/4c//EEuXLhgWmp5+jlO77zzjgmya9WqJS+++KL8/ve/lx9++EFuueUW13gmv/jFL0yQ+49//KPE+3Wf3i9Npzv6MF27jOnn/PKXv5QXXnhBfv75ZxN4X0qvUR8eaJms916vX4PpIUOGuL2u3/72t/Ltt9/K1KlT5dFHHzX5YcyYMSUCdL2+06dPmwcEf/zjH6Vjx46yevVq1zHaSqJHjx6SkZFhzjV9+nTzUEAf0Gzfvr1c9xMoNwcAn3bu3DmH/lMdPHiwx+/Jzs4usa9///6Oq6++2rX9wQcfmPN+9dVXpZ5n8+bN5phFixYV27969eoS+9944w2zT9eXM3LkSHNs3bp1HXfffbdj9uzZjj179pQ4Li0tzRznybJhwwbX+6644grH448/7vDUrFmzzDn08wAAvqM6loHNmjUzx27atMm17/jx446wsDDHuHHjXPueeuopc5ymw+n8+fOOFi1aOJo3b+4oKCi47Gc531OnTh1HUlJSsf3p6emmvCy6v2vXro6EhIRix23fvt2k4+2333btGzVqlLkOpxUrVphjZs6c6dqXn5/v6N69e4n70rt3b0eHDh0cFy5ccO0rLCx03HzzzY5rrrmmxD3t06ePed3p6aefdgQFBTnOnj1rtnVdu3ZtR2JioiMnJ6dY2p3v07WeW/NB0XNpXtH72bdvX4/uJVBR1HQDPk6fyKryNIXWWl0nbeqmNcHahEqbqTmbvmnTL7Vy5cpiT+uL0tpobYKlT+KL1ionJCSYp+UbNmxwHatP6PVJt64vR5v+/eUvfzFN+fSpttZEaw1B79695fDhw67jYmNjZe3atR4t8fHxrvfptWkTsiNHjnh8zwAAvqc6loFKm7xrU28nbTrdunVrk0anjz/+2DQ/19poJ/3cX//616Z2WmuqPaFlpNbojhgxoth1BAUFmVlEil7H8OHDJSUlxYwU77R06VLTTF9rskujadVBUbUm2knPr7XURWlNtNY4a626dgVzpuXUqVOm9cC///3vYr8DlF6v1sA76X0rKChwNYPX69Nzaa25NlUvyvk+HSxVz33//febz3J+blZWlvntsWnTJgaGg60YSA3wcdovSWmB4intn6RNp3SE7kubcOsPDv0RoT9AtPn1tGnT5E9/+pPceuutpmmXFkhauCotoPT40vpZa/O8iggMDDSD3+iihZ+md/78+aY/lg6Ws3nzZnOcFp59+vSpUP8/bdKmTQr1x9Edd9xhRrvV5okAAP9RHctA1bRp0xL7tOl20b7iGlRqUHwpfUjtfF1Hcb8cvQ6lzajLusdK+49r/2kNtLV5vj5I0IcPAwYMKHbcpTQt2t9eHwoUpQ8SLm1Wr+fU5u26lHZf4+LiSr1XzibuznvlfEBQ1r1w3gN3zd2d9G9dWvN5oLIIugEfp4XcVVddJbt37/boeC189KltmzZtJDk52QSeOviJPoXWHxbOKT706e+yZctM/zXtH6V9xnQAGe1bpfu04NRj9ceG9l9zR5/MV1b9+vVN/zRd9EfPZ599Zgpv7Y+nT7I9Hc1cB2PR61T6BF2fhGst+po1a2TWrFmmD9vy5cvNDwcAgH+ormWg1gK7owGp1ZzXrP2ttQXZpbSG2knvtZaf2odbg269F4cOHTJlqJVp0RZuWrPtjg6MZ/W9cn6u/h7Qvt7uXPrAALASQTfgB3SUU51eS5/ad+3atcxj9cdDbm6uGQW06NPhos3HitLBTHTRAVp0lE8dSGbJkiXyyCOPSMuWLeWf//ynGUCmaHM9u+jopRp0Hz161ATdP/30k2mC7gm9Pg3anfSJuw48o4s+Ne/UqZO5RoJuAPAvNaUMvJSWg6mpqW5H/na+7gm9DqUPEDxpPaZNzLXs1M/WGm8d5fvOO++8bFrXrVtnRpkvGrxemn5nizOdcqwiLdnKuj59MHNpwH7pMfoQx6rPBcqDPt2AH/jd735nRvbUHwE6mqm7J/s6qmrRJ8JFnwBrkyntR12UNsu69Cmx8+mv/mBx1hhrbfPzzz9f4jPz8/OLTQXi6XQp6enpbvuhXbx40RTY2vTcWWhWpE+3pvfSNOgPDX1677wuAID/qE5lYHlo1ygdVVsfNjhpH2R9AKHTdWm/cE9ojbIGmzpat7v+65e2KNNm93ofFy9ebJqW60MPvf+XS6vek6JTs+m901HhLy2P9QH5a6+9Zh6wXy4tntBR1bXP/4wZM8zo7kU5/8ba1UwDb50WTR8MWPG5QHlQ0w34AS0o9Am8Pn3WvlzaP1n7Lmmg+sUXX5hC0Tl4ixY+2pROn0rrVCdauCxYsMAUdEULuLfeestMp6XzXer5tb+cHqcFsxaeSvu86Tm0INNBSPTc+nRa+0bpZ+qPHJ3yRGlTbp1KRX/YlDWQjE4hogPDaN8ybQKogbXWRGvhrlOC6PzaOmdqRft063U0btzYpEsDcX3irjUVX331lWk26KQ/jJw/BpxzdOrgbjq4ji6XTkcCAPCO6lQGlocODKZlo7bQeuKJJ0w3Kk13WlqavP/+++YhtSf0mjQYfvDBB02rLx07RZvGa7NxnfNba/K1/HPSe9WrVy/TPF/vi973y9H7refRNOsgb/pAQLt0uXsIoVO36eBwHTp0kKSkJFP7rQ9T9OGC/kbQ3wLlodenXQf0oYzOza398rVvtp5H+/TrPdN79de//tXcS51zXf9W2m9cB23TVhB6Dm0lAdimwuOeA6hy//rXv8zUHjpVSGhoqJkio1u3bo6XX3652NQbH330keP66693hIeHm2NffPFFx8KFC4tNi7Vz507HiBEjHE2bNjXTlDRo0MAxaNAgx44dO0p87uuvv26mEImIiDCfqVN9/O53v3McOXKk3NOlZGRkOF566SUzbUfjxo0dISEh5pw6TcmCBQuKTeVREbm5uY4JEyY44uPjzXmjoqLM/7/yyiseT0dWdBoUAIBvqA5loNIyZuDAgSX29+zZ0yxF7d+/3zF06FAz5ZdeT5cuXRwrV650VIROrallr04Tpudq2bKl46GHHnJ7zVoe6/Xo9V46DZe7KcPUqVOnHA8++KAjOjrafIb+/9dff+32vuh16fShsbGx5ndAXFycuf/Lli0rcU8vndZNr+PSqUKdf3eddkz/TpoGvVeLFy8udoym55577nHUr1/f/N31Gn7xi1841q1bV867CZRPgP7HvpAeAAAAAICaiz7dAAAAAADYhD7dAAAAgB/SAcB0wLLSaP927QsOwLtoXg4AAAD4IR3F/ODBg6W+roPBbdy4sUrTBKAkaroBAAAAP7Ro0SLJyckp9XUdxRuA91HTDQAAAACATRhIDQAAAAAAm9S45uWFhYVy5MgRqV27tgQEBHg7OQAAWEJnAD1//rxcddVVEhjoW8/UKXsBADW57K1xQbcG3E2aNPF2MgAAsMVPP/0kjRs39qm7S9kLAKjJZW+NC7q1htt5Y6Kjo72dHJ+Vl5cna9askX79+klISIi3kwM/RT4CeanqZGRkmIfKznLOl1D2eobvTFiBfASrkJesK3trXNDtbFKuATdBd9n/yCIjI809IuhGRZGPYBXykud8sesUZa9nyOewAvkIViEvWVf2+lanLwAAAAAAqhGCbgAAAAAAbELQDQAAAACATQi6AQAAAACozkH3vHnzpHnz5hIeHi6JiYmyffv2Uo+99dZbTUf1S5eBAwdWaZoBAAAAAPD5oHvp0qUyduxYmTp1quzcuVPi4+Olf//+cvz4cbfHL1++XI4ePepadu/eLUFBQTJs2LAqTzsAAIAVCgoK5LPPPpNNmzaZtW4DAKoHrwfdycnJkpSUJKNHj5Z27drJ/PnzzVRVCxcudHt8vXr1JDY21rWsXbvWHE/QDQAA/JFWKLRq1Ur69u1rfhfpWrd1PwDA/3l1nu6LFy9KSkqKTJo0ybUvMDBQ+vTpI1u3bvXoHH/729/kvvvuk6ioKLev5+bmmqXoBObOeed0gXvOe8M9QmWQj2AV8pLn98gXUPZ67oMPPjC/Y+644w554403JD093VQqzJ49W4YOHSpLliyRu+++28a/Fqobvi9BXvK9sterQffJkydN86mGDRsW26/be/fuvez7te+3Ni/XwLs0M2bMkGnTppXYv2bNGlNDjrJpSwKgsshHsAp5qXTZ2dk+k9Eoez2jv4F++9vfSufOneXhhx+Wc+fOSUREhFnrtna1e+KJJyQ4ONh0pQPKg+9LWIW8VPmy16tBd2VpsN2hQwfp0qVLqcdoLbr2GS9a092kSRPp16+fREdHV1FK/fOpjf4D0yZuISEh3k4O/BT5COSlquNsyeULKHs9o323NbB+//33zUCyl35nxsTESI8ePczvlZ49e9r8V0N1QdkL8pLvlb1eDbq1MNEnt8eOHSu2X7e1aVVZsrKyTJOr5557rszjwsLCzHIpLcwIJi+P+wQrkI9gFfJS2ffGV1D2eubEiRNm3bFjx2J/P2c+1/3O43zp7wv/wPclyEv28/S72asDqYWGhkpCQoKsW7fOta+wsNBsd+3atcz3vvfee6bP2C9/+csqSCkAAIC1GjVqZNbaVc4d537ncQAA/+T10cu16feCBQvkrbfekj179sijjz5qarF1NHM1cuTIYgOtFW1aPmTIEKlfv74XUg0AAFA53bt3l+bNm8v06dNNpUNRuq1941u0aGGOAwD4L6/36R4+fLhpNjVlyhQzYqc2pVq9erVrcLVDhw6ZEc2LSk1NlS1btpjB0AAAAPyRdrGbM2eOGaVcKxImTJggOTk5sm3bNpk1a5asXLlSli1bxiBqAODnvB50qzFjxpjFnY0bN5bY17p1a3E4HFWQMgAAAPvcc889JrAeN26cGTTNSWu4db++DgDwbz4RdAMAANRUGlgPHjxYNmzYIJ988okMGDBAevXqRQ03AFQTBN0AAAA+0NRcpwXTcW10zbzcAFB9eH0gNQAAAAAAqiuCbgAAAAAAbELQDQAAAACATQi6AQAAAACwCUE3AAAAAAA2IegGAAAAAMAmBN0AAAAAANiEoBsAAAAAAJsQdAMAAAAAYBOCbgAAAAAAbELQDQAAAACATQi6AQAAAACwCUE3AAAAAAA2IegGAAAAAMAmBN0AAAAAANiEoBsAAAAAAJsQdAMAAAAAYBOCbgAAAAAAbELQDQAAAACATQi6AQAAAACwCUE3AAAAAAA2IegGAAAAAMAmBN0AAAAAANiEoBsAAAAAAJsQdAMAAAAAYBOCbgAAAAAAqmvQPW/ePGnevLmEh4dLYmKibN++vczjz549K48//rg0atRIwsLC5Nprr5WPP/64ytILAAAAAICngsWLli5dKmPHjpX58+ebgHvu3LnSv39/SU1NlQYNGpQ4/uLFi9K3b1/z2rJlyyQuLk4OHjwoderU8Ur6AQAAAADw2aA7OTlZkpKSZPTo0WZbg+9Vq1bJwoULZeLEiSWO1/2nT5+WL774QkJCQsw+rSUHAAAAAMAXea15udZap6SkSJ8+ff6TmMBAs71161a37/noo4+ka9eupnl5w4YNpX379jJ9+nQpKCiowpQDAAAAAODjNd0nT540wbIGz0Xp9t69e92+58cff5T169fLAw88YPpx79u3Tx577DHJy8uTqVOnun1Pbm6uWZwyMjLMWt+jC9xz3hvuESqDfASrkJc8v0e+gLK3YsjnsAL5CFYhL1lX9nq1eXl5FRYWmv7cr7/+ugQFBUlCQoIcPnxYZs2aVWrQPWPGDJk2bVqJ/WvWrJHIyMgqSLV/W7t2rbeTgGqAfATykv2ys7N9JqNR9lYO35mwAvkIViEvVb7s9VrQHRMTYwLnY8eOFduv27GxsW7foyOWa19ufZ9T27ZtJT093TRXDw0NLfGeSZMmmcHaitZ0N2nSRPr16yfR0dGWXlN1e2qj/8B04Dpn/3mAfARv4Tvp8pwtuXwBZW/FkM9hBfIRrEJesq7s9VrQrQGy1lSvW7dOhgwZ4qrJ1u0xY8a4fU+3bt3k3XffNcdp/2/1r3/9ywTj7gJupdOK6XIpDSQJJi+P+wQrkI9gFfJS2ffGV1D2lp92udOBYjdt2iRRUVHSq1evYpUMQHnxfQmrkJcqX/Z6dZ5urYFesGCBvPXWW7Jnzx559NFHJSsryzWa+ciRI83Tcid9XUcvf/LJJ02wrSOd60BqOrAaAACAP1q+fLm0atXKtC7TmV10rdu6HwDg/7zap3v48OFy4sQJmTJlimki3rFjR1m9erVrcLVDhw65arSVNgv/9NNP5emnn5brr7/ezNOtAfgzzzzjxasAAACoGA2shw4dKoMGDZJ33nlHfv75Z2ncuLHMnDnT7F+2bJncc8893F4A8GNeH0hNm5KX1px848aNJfbplGHbtm2rgpQBAADY26R83LhxJuBesWKF2T516pQkJiaabe1+N378eBk8eDBNzQHAj3m1eTkAAEBNtXnzZjlw4IBMnjy5WMs+pdvaxS4tLc0cBwDwXwTdAAAAXnD06FGzbt++vdvXnfudxwEA/BNBNwAAgBfo7Ctq9+7dbl937nceBwDwTwTdAAAAXtC9e3dp3ry5mYlFp0MtSrdnzJghLVq0MMcBAPwXQTcAAIAX6Dzcc+bMkZUrV5pB03Sg2JycHLPWbd0/e/ZsBlEDAD/n9dHLAQAAaiqdDkynBdNRzHv06OHarzXcTBcGANUDQTcAAICXA2+dFmzDhg3yySefyIABA6RXr17UcANANUHQDQAA4ANNzXv27ClZWVlmrdsAgOqBPt0AAAAAANiEoBsAAAAAAJsQdAMAAAAAYBOCbgAAAAAAbMJAajVIdna27N2716NjM3Ny5Ytd+6VuzA6pFRHm0XvatGkjkZGRlUwlAAAAAFQfBN01iAbcCQkJ5XrPzHIcm5KSIp06dSp3ugAAAACguiLorkG0JloDY0+kHj0rY9/bJcnDOkjrRnU8Pj8AAAAA4D8IumsQbfrtaU104MFTErY5R9q2j5eOzerbnjYAAAAAqI4YSA0AAAAAAJsQdAMAAAAAYBOCbgAAAC8rKCiQzz77TDZt2mTWug0AqB4IugEAALxo+fLl0qpVK+nbt68kJyebtW7rfgCA/yPoBgAA8BINrIcOHSodOnSQzZs3y+LFi81at3U/gTcA+D+CbgAAAC/QJuTjxo2TQYMGyYoVKyQxMVEiIiLMWrd1//jx42lqDgB+jqAbAADAC7RG+8CBAzJ58mQJDCz+k0y3J02aJGlpaeY4AID/IugGAADwgqNHj5p1+/bt3b7u3O88DgDgnwi6AQAAvKBRo0ZmvXv3brevO/c7jwMA+CeCbgAAAC/o3r27NG/eXKZPny6FhYXFXtPtGTNmSIsWLcxxAAD/RdANAADgBUFBQTJnzhxZuXKlDBkyRLZt2yY5OTlmrdu6f/bs2eY4AID/CvZ2AgAAAGqqe+65R5YtW2ZGMe/Ro4drv9Zw6359HQDg3wi6AQAAvEgD68GDB8uGDRvkk08+kQEDBkivXr2o4QaAasInmpfPmzfP9GkKDw83c1Nu37691GPffPNNCQgIKLbo+wAAAPyVNiHv2bOnqe3WNU3KAaD68HrQvXTpUhk7dqxMnTpVdu7cKfHx8dK/f385fvx4qe+Jjo4202c4l4MHD1ZpmgEAAAAA8IugOzk5WZKSkmT06NHSrl07mT9/vkRGRsrChQtLfY/WbsfGxrqWhg0bVmmaAQAAAADw+aD74sWLkpKSIn369PlPggIDzfbWrVtLfV9mZqY0a9ZMmjRpYvpAff/991WUYgAAAAAA/GQgtZMnT0pBQUGJmmrd3rt3r9v3tG7d2tSCX3/99XLu3DkzlcbNN99sAu/GjRuXOD43N9csThkZGWadl5dnFriXn5/vWnOfUFHOvEMeQmWRlzy/R76AsrdiyOewAvkIViEvWVf2+t3o5V27djWLkwbcbdu2lddee02ef/75EsfPmDFDpk2bVmL/mjVrTDN2uPdTpv432MwVeng3dwmVs3btWm4hLEFeKl12drbP5DLK3sohn8MK5CNYhbxU+bI3wOFwOMSLzcs18NV5KIcMGeLaP2rUKDl79qx8+OGHHp1n2LBhEhwcLIsXL/boabs2S9dadh2QDe59e+i0DF2wQ5YldZb4pvW4Tajw0z/9ou7bt6+EhIRwF1Fh5KXL0/ItJibGtALzdvlG2Vsx5HNYgXwEq5CXrCt7vVrTHRoaKgkJCbJu3TpX0F1YWGi2x4wZ49E5tHn6rl275I477nD7elhYmFkupQEAQUDp9CGGc819QmXx7w1WIS+VfW98BWVv5ZDPYQXyEaxCXqp82ev15uU6XZjWbHfu3Fm6dOkic+fOlaysLDOauRo5cqTExcWZpmrqueeek5tuuklatWplasNnzZplpgx75JFHvHwlAAAAAAD4WNA9fPhwOXHihEyZMkXS09OlY8eOsnr1atfgaocOHTIjmjudOXPGTDGmx9atW9fUlH/xxRdmujEAAAAAAHyJ14NupU3JS2tOvnHjxmLbf/rTn8wCAAAAAICv8+o83QAAAAAAVGcE3QAAAAAA2ISgGwAAAAAAmxB0AwAAAABgE4JuAAAAAABsQtANAAAAAIBNCLoBAAAAALAJQTcAAAAAADYh6AYAAAAAwCYE3QAAAAAA2CTYrhMDAADUdNnZ2bJ3716Pjs3MyZUvdu2XujE7pFZEmEfvadOmjURGRlYylQAAOxF0AwAA2EQD7oSEhHK9Z2Y5jk1JSZFOnTqVO10AgKpD0A0AAGATrYnWwNgTqUfPytj3dknysA7SulEdj88PAPBtBN0AAAA20abfntZEBx48JWGbc6Rt+3jp2Kw+fxMAqCYYSA0AAAAAAJsQdAMAAAAAYBOCbgAAAAAAbELQDQAAAACATQi6AQAAAACwCUE3AAAAAAA2IegGAAAAAMAmBN0AAAAAANgk2K4TAwAAVFdpJ7MkKzff0nPuP5HlWgcHW/sTLSosWFrERFl6TgCAZwi6qwEKfgAAqrbc7TV7o23nH7dsly3n3TD+VgJvAPACgm4/R8EPAEDVctZwzx3eUVo1qGXdeXNyZeXGrTLo1q4SFRFm2Xn3Hc+Up5Z+Y3nNPADAMwTdfo6CHwAA79CAu33cFZadLy8vT9KvFOnUrK6EhIRYdl4AgHcRdFcTFPwAAAAA4HsYvRwAAAAAAJsQdAMAAAAAUJ2D7nnz5knz5s0lPDxcEhMTZfv27R69b8mSJRIQECBDhgyxPY0AAAAAAPhd0L106VIZO3asTJ06VXbu3Cnx8fHSv39/OX78eJnvO3DggIwfP166d+9eZWkFAAAAAMCvgu7k5GRJSkqS0aNHS7t27WT+/PkSGRkpCxcuLPU9BQUF8sADD8i0adPk6quvrtL0AgAAAADgF6OXX7x4UVJSUmTSpEmufYGBgdKnTx/ZunVrqe977rnnpEGDBvLwww/L5s2by/yM3NxcszhlZGS4puXQxd/l5+e71lZej/NcVt8ju9IL32RXPkLNQ17y/B75AsreiqHshRX4voRVyEvWlb1eDbpPnjxpaq0bNmxYbL9u79271+17tmzZIn/729/km2++8egzZsyYYWrEL7VmzRpTo+7vfsrU/wab+3KwlvXnX7t2rV+lF77J6nyEmou8VLrs7GzxFZS9lUPZCyvwfQmrkJcqX/b61Tzd58+flwcffFAWLFggMTExHr1Ha9G1z3jRmu4mTZpIv379JDo6Wvzd90cyZPaubXLLLbfIdVdFW/rURv+B9e3bV0JCQnw+vfBNduUj1DzkpctztuTyBZS9FUPZCyvwfQmrkJesK3vLFXQXFhbKm2++KcuXLzcDmenI4S1atJChQ4eaYFi3y0MD56CgIDl27Fix/bodGxtb4vj9+/ebz73zzjuLpclcSHCwpKamSsuWLYu9JywszCyX0gCgOgQBet3OtR3XY/V9sju98E3V5d8bvI+8VPa9qSyrynnK3sqh7IUV+L6EVchLlS97PR5IzeFwyF133SWPPPKIHD58WDp06CDXXXedHDx4UB566CG5++67pbxCQ0MlISFB1q1bV6zA1+2uXbuWOL5Nmzaya9cu07TcuWiaevXqZf5fa7ABAED52VHOAwCActR065PvTZs2mYBYg9yi1q9fb+bKfvvtt2XkyJHluq/a9HvUqFHSuXNn6dKli8ydO1eysrLMaOZKzxcXF2f6h+k83u3bty/2/jp16pj1pfsBAIDn7CrnAQCo6Tyu6V68eLFMnjy5REGsbrvtNpk4caIsWrSo3AkYPny4zJ49W6ZMmSIdO3Y0NdarV692Da526NAhOXr0aLnPCwAAPGdXOQ8AQE3ncdD93Xffye23317q6wMGDJBvv/22QokYM2aMab6mU4x8+eWXkpiY6Hpt48aN5ul7afS1FStWVOhzAQCA/eU8AAA1mcfNy0+fPl1iaq+i9LUzZ85YlS4AAFCFKOfLJyA4Q9IyUiUw3Lr5L/Pz8+VI/hHZc3qPa+BRK6RlZJr0AgC8w+NvdJ1Pu6wCQEch18ICAAD4H8r58gmp86VM3j7dlr/FK6tfsfycIXV6i8gdlp8XAGBh0K2jmurope6m31LaNBwAAPgnyvnyyTubKHMG3i8tG1hb0/35ls+l2y3dLK3p3n88U55YtN+y8wEAysfjb3QdrfRy83MyoikAAP6Jcr58HPnR0iK6tbSrf4Vlf4O8vDxJC06TtvXaWjLvulPhhXPiyD9h2fkAADZOGQYAAKonynkAALw8evnQoUPNVF7a/AwAAFQvlPMAAHg56NaRyQcOHChNmzY1c2r/+OOPNiUJAABUNcp5AAC8HHSvW7fOBNoPP/yw/P3vf5drrrlGbrvtNnn33XcZRA0AAD9HOQ8AgD3KNTRms2bN5NlnnzXL+vXrZeHChZKUlCRjxoyRESNGyK9+9StJSEiwKakAAMBOlPOeyckrMOvdh89Zev+zcnJlxwmR2INnJCrC/WwxFbHveKZl5wIAlF+F56PQWm5dzp8/b2q7J0+eLK+99hpzdQMAUA1Qzpc9BZeauHyXDXc+WN7Z95UN5xWJCrNuGjIAgOcq9e2blpZmRjvV5dy5c9KnT5/KnA4AAPgQynn3+l0Xa9Y6R3dESJBl9zv16DkZt2yXzBnaQVo3sm4qMmfA3SImytJzAgBsCrovXLggy5YtM03LN23aJE2aNDH9vEePHm3+HwAA+C/K+curFxUq93Vpavm9z8/PN+uWV0ZJ+zhrg24AgB8E3du3bzeB9tKlS02BfPfdd5spxHr37i0BAQH2phIAANiKch4AAC8H3TfddJPEx8fL888/Lw888IDUrVvXpiQBAICqRjkPAICXg+5BgwbJkiVLJDIy0qakAAAAb6GcBwDAy0H3qlWrJDMzk6DbBwUEZ0haRqoEhteytF/Zkfwjsuf0HgkOtm6007SMTJNeAIBvoZwHAMAeHkdTDofDpiSgskLqfCmTt0+35Ua+svoVy88ZUqe3iNxh+XkBABVHOQ8AgD3KVYXJgGm+Ke9soswZeL+ZusTKmu7Pt3wu3W7pZmlNt85t+sSi/ZadDwBgHcp5AACsV65o6tprr71sgXz69OnKpgnl5MiPlhbRraVdfeumF8nLy5O04DRpW6+thISEWHbewgvnxJF/wrLzAQCsQzkPAICXg+5p06bJFVcwbyQAANUR5TwAAF4Ouu+77z5p0KCBDckAAADeRjkPAID1Aj09kH5eAABUX5TzAAB4OehmVFMAAKovynkAALzcvLywsNCmJAAAAG+jnAcAwMs13QAAAAAAoHwIugEAAAAAsAlBNwAAAAAANiHoBgAAAADAJgTdAAAAAAB4e/RyO82bN09mzZol6enpEh8fLy+//LJ06dLF7bHLly+X6dOny759+yQvL0+uueYaGTdunDz44INSE+XkFZj17sPnLD1vVk6u7DghEnvwjERFhFl23n3HMy07FwAAAAD4Oq8H3UuXLpWxY8fK/PnzJTExUebOnSv9+/eX1NRUadCgQYnj69WrJ//93/8tbdq0kdDQUFm5cqWMHj3aHKvvq2n2//8gduLyXTacPVje2feVDecViQrzetYDAMB22dnZsnfvXo+OTT16VnLT98me3RFSeKqOR+/R30ORkZGVTCUAwE5ej3ySk5MlKSnJBM5Kg+9Vq1bJwoULZeLEiSWOv/XWW4ttP/nkk/LWW2/Jli1bamTQ3e+6WLNu2aCWRIQEWXbe1KPnZNyyXTJnaAdp3egKsTrgbhETZek5AQDwRRpwJyQklOs997/l+bEpKSnSqVOn8icMAFAzgu6LFy+awmLSpEmufYGBgdKnTx/ZunXrZd/vcDhk/fr1plb8xRdfdHtMbm6uWZwyMjLMWpum6+LvaocGyL03NLL8vBcuXDDrZnXDpHUD65+gV4d7D8//zvy9UVnkJc/vkS+o7mVvebRs2VK+/PJLj47NzMmVTzd/Jf273yi1POzapeevafcUZeP7ElYhL12ep9+/Xg26T548KQUFBdKwYcNi+3W7rKZY586dk7i4OFOgBwUFySuvvCJ9+/Z1e+yMGTNk2rRpJfavWbOG5lhl+Mm0Wg+Wbdu2yeHdHv9JAbfWrl3LnYElyEtlN2P2FZS9FXdzh5Zy/uxpOX/Ws+OPHj1aiU9Ddcb3JchLvlP2er15eUXUrl1bvvnmG8nMzJR169aZPuFXX311iabnSmvR9fWiT9ubNGki/fr1k+jo6CpOuf/49tBpkV075KabbpL4pvW8nRz48dM/LfT1oVhISIi3kwM/Rl66PGdtsi+g7K0Y8jmsQD6CVchL1pW9Xg26Y2JiTE31sWPHiu3X7djY/+ur7I42QW/VqpX5/44dO8qePXvMU3V3QXdYWJhZLqUBAEFA6YKDg11r7hMqi39vsAp5qex74ysoeyuHfA4rkI9gFfJS5cter87TraOP6+AiWlvtVFhYaLa7du3q8Xn0PUX7jgEAAAAA4Au83rxcm36PGjVKOnfubObm1inDsrKyXKOZjxw50vTf1ppspWs9VgcO0UD7448/lnfeeUdeffVVL18JAAAAAAA+FnQPHz5cTpw4IVOmTJH09HTTXHz16tWuwdUOHTpkmpM7aUD+2GOPyc8//ywRERFmfsq///3v5jwAAAAAAPgSrwfdasyYMWZxZ+PGjcW2X3jhBbMAAAAAAODrvNqnGwAAAACA6swnarpRdfPIlTX/eVGpR89Kbvo+2bM7QgpP1fHoPdrUPzIyspKpBAAAAIDqg6C7BtGAW0eLL4/73/L82JSUFOnUqVP5EwYAAAAA1RRBdw2iNdEaGHsiMydXVm3YKgN7dZVaEWEenx8AAAAA8B8E3TWINv32tCY6Ly9Pzpw8Ll27dPZ40ncAAAAAQHEMpAYAAAAAgE0IugEAAAAAsAlBNwAAAAAANiHoBgAAAADAJgTdAAAAAADYhKAbAAAAAACbEHQDAAAAAGATgm4AAAAAAGxC0A0AAAAAgE0IugEAAAAAsAlBNwAAAAAANiHoBgAAAADAJgTdAAAAAADYhKAbAAAAAACbEHQDAAAAAGATgm4AAAAAAGxC0A0AAAAAgE0IugEAAIBqoKCgQD777DPZtGmTWes2AO8j6AYAAAD83PLly6VVq1bSt29fSU5ONmvd1v0AvIugGwAAAPBjGlgPHTpUOnToIJs3b5bFixebtW7rfgJvwLsIugEAAAA/pU3Ix40bJ4MGDZIVK1ZIYmKiREREmLVu6/7x48fT1BzwIoJuAAAAwE9pjfaBAwdk8uTJEhhY/Ke9bk+aNEnS0tLMcQC8I9hLnwvAT2VnZ8vevXs9OjYzJ1e+2LVf6sbskFoRYR69p02bNhIZGVnJVAIAUDMcPXrUrNu3b+/2ded+53EAamjQPW/ePJk1a5akp6dLfHy8vPzyy9KlSxe3xy5YsEDefvtt2b17t9lOSEiQ6dOnl3o8AGtpwK3/7spjZjmOTUlJkU6dOpU7XQAA1ESNGjUya/1tfNNNN5V43fmb2XkcgBoYdC9dulTGjh0r8+fPN31P5s6dK/3795fU1FRp0KBBieM3btwoI0aMkJtvvlnCw8PlxRdflH79+sn3338vcXFxXrkGoCbRmmgNjD2RevSsjH1vlyQP6yCtG9Xx+PwAAMAz3bt3l+bNm5tKKO3DXVRhYaHMmDFDWrRoYY4DUEODbp3SICkpSUaPHm22NfhetWqVLFy4UCZOnFji+EWLFhXb/utf/yrvv/++rFu3TkaOHFll6QZqKm367WlNdODBUxK2OUfato+Xjs3q2542AABqmqCgIJkzZ44ZpXzIkCEyYcIEycnJkW3btpmWpCtXrpRly5aZ4wDUwKD74sWLpsZMB3goOuBDnz59ZOvWrR73L83Ly5N69erZmFIAAADAN91zzz0msNZRzHv06OHarzXcul9fB1BDg+6TJ0+a6QsaNmxYbL9uezpQ0zPPPCNXXXWVCdTdyc3NNYtTRkaGWWugrgvcc94b7hEqIz8/37UmL6Ey+E7y/B75AsreiiGfozLuvPNOueOOO0xXzLVr10rfvn3l1ltvNTXcvvT9AP/Bd9Llefpvy+vNyyvjj3/8oyxZssR8uWj/bne0H8u0adNK7F+zZg0jJHtAv7SBivopU/8bbJq4Hf6/cVyASuE7qeyWX76CsrdyyOeoLK3t1odfn376KTcTlcZ3UuXL3gCHw+EQLzYv1/6h2uxF+6A4jRo1Ss6ePSsffvhhqe+dPXu2vPDCC/LPf/5TOnfuXK6n7U2aNDG17NHR0RZeTfV7auN8ShoSEuLt5MBPfXvotAxdsEOWJXWW+KZ0AUHF8Z10eVq+xcTEyLlz57xevlH2Vgz5HFYgH8Eq5CXryl6v1nSHhoaaqYd0EDRn0K2jLOr2mDFjSn3fzJkz5Q9/+IN5eldWwK3CwsLMcikNJAkmL4/7hMoIDg52rfn3BivwnVT2vfEVlL2VQz6HFchHsAp5qfJlr9ebl+t0YVqzrcGzzrWtU4ZlZWW5RjPXEcl1KjBtqqZ0irApU6bIu+++a6ZH0Lm9Va1atcwCAAAAVLcmrJ6Od5SZkytf7NovdWN2SK2IkhVPpU3Xqa1PAdjD60H38OHD5cSJEyaQ1gC6Y8eOsnr1atfgaocOHTIjmju9+uqrplm6TotQ1NSpU+XZZ5+t8vQDAAAAdtKAW1uHlsfMchyrswl5Oh0oAD8MupU2JS+tObkOklbUgQMHqihVAAAAgPdpTbQGxp5IPXpWxr63S5KHdZDWjep4fH4A1TzoBgAAAOCeNv32tCY68OApCducI23bx0vHZvW5pYAP+E+7bQAAAAAAYCmCbgAAAAAAbELQDQAAAACATQi6AQAAAACwCUE3AAAAAAA2YfRyAAAAwAvSTmZJVm6+pefcfyLLtQ4OtvanflRYsLSIibL0nEBNQNANwKDgBwCgasvdXrM32nb+cct22XLeDeNvJfAGyomgGwAFPwAAVcxZwz13eEdp1aCWdefNyZWVG7fKoFu7SlREmGXn3Xc8U55a+o3lNfNATUDQDYCCHwAALwgIzpCg8MMSGG5d0B0RnC9X1T0iEbXTJdDC5uVB4ZkmvQDKj6AbgIs+aW8fd4VldyQvL0/SrxTp1KyuhISEcKcBACgipM6XMnn7dFvuySurX7H8nCF1eovIHZafF6juCLoBAAAAL8g7myhzBt4vLS1sXp6fny+fb/lcut3SzdKB1PYfz5QnFu237HxATULQDQAAAHiBIz9aWkS3lnb1rW1llhacJm3rtbW0lVnhhXPiyD9h2fmAmoR5ugEAAAAAsAk13QAMHRwlLSPV0sFctInbkfwjsuf0HkubuKVlMJgLAAAA/ANBNwCDwVwAAAAA6xF0AzAYzAUAgKqTk1dg1rsPn7P0vDpP944TIrEHz1g+TzeAiiHoBmAwmAsAAFVHRwNXE5fvsuHswfLOvq9sOK9IVBjhA1Be/KsBAAAAqli/62LNWqcLiwgJsuy8qUfPybhlu2TO0A7SupF1o6I7A+4WMVGWnhOoCQi6AQAAgCpWLypU7uvS1PLz6iCmquWVUdI+ztqgG0DFMGUYAAAAAAA2IegGAAAAAMAmBN0AAAAAANiEPt0AmLYEAAAAsAlBNwCmLQEAAABsQtANgGlLAADwYdnZ2bJ3716Pjk09elZy0/fJnt0RUniqjkfvadOmjURGRlYylQBKQ9ANgGlLAADwYRpwJyQklOs997/l+bEpKSnSqVOn8icMgEcIugEAAAAfpjXRGhh7IjMnV1Zt2CoDe3WVWhFhHp8fgH0IugEAAAAfpk2/Pa2JzsvLkzMnj0vXLp0lJCTE9rQB8IMpw+bNmyfNmzeX8PBwSUxMlO3bt5d67Pfffy/33nuvOT4gIEDmzp1bpWkFAAAAAMBvarqXLl0qY8eOlfnz55uAW4Po/v37S2pqqjRo0MDtIBJXX321DBs2TJ5++mmvpBmo6RjMBQAAAPCToDs5OVmSkpJk9OjRZluD71WrVsnChQtl4sSJJY6/8cYbzaLcvQ7AfgzmAgAAAPhB0H3x4kUzIMSkSZNc+wIDA6VPnz6ydetWbyULwGUwmAsAAADgB0H3yZMnpaCgQBo2bFhsv257Og+hJ3Jzc83ilJGR4RpkQhe457w33CNcSgdl6dChQ7kGc+l8Q3y5BnMh34HvpPLzpX83lL0VQ9kLK5CPYBXyknVlb7UfvXzGjBkybdq0EvvXrFljRoJE2dauXcstQqWRj2AV8lLZ4y34CsreyiGfwwrkI1iFvFT5stdrQXdMTIwEBQXJsWPHiu3X7djYWMs+R5uv62BtRWu6mzRpIv369ZPo6GjLPqc6PrXRf2B9+/ZlugmQj+B1fCddnrMlly+g7K0Y8jmsQD6CVchL1pW9Xgu6Q0NDJSEhQdatWydDhgwx+woLC832mDFjLPucsLAws1xKm7oyd+HlcZ9gBfIRrEJeKvve+ArK3sohn8MK5CNYhbxU+bLXq83LtQZ61KhR0rlzZ+nSpYuZMiwrK8s1mvnIkSMlLi7ONFNzDr72ww8/uP7/8OHD8s0330itWrWkVatW3rwUAAAAAAB8K+gePny4nDhxQqZMmSLp6enSsWNHWb16tWtwtUOHDpkRzZ2OHDkiN9xwg2t79uzZZunZs6ds3LjRK9cAAAAAAIDPDqSmTclLa05+aSDdvHlzcTgcVZQyAAAAAAAq5z/VyAAAAAAAwFIE3QAAAAAAl4KCAvnss89k06ZNZq3bqDiCbgAAAACAsXz5cjNItU4dnJycbNa6rftRMQTdAAAAAAATWA8dOlSOHTtW7G7otu4n8PbTgdQAADVTdna27N2716NjM3Ny5Ytd+6VuzA6pFRHm0XvatGkjkZGRlUwlAAA1gzYhf/TRR83A1b1795ZnnnlGfv75Z2ncuLG8+OKLsnLlSvP64MGDJSgoyNvJ9SsE3QAAr9CAOyEhoVzvmVmOY1NSUqRTp07lThcAADWRzhx1/PhxueWWW+TDDz80QfipU6ckMTHRbPfo0UM+//xzc5wG5fAcQTcAwCu0JloDY0+kHj0rY9/bJcnDOkjrRnU8Pj8AAPCMc7rmadOmSWBgYLHB03T72WefNf27CbrLj6AbAOAV2vTb05rowIOnJGxzjrRtHy8dm9W3PW0AAABWIegGAAAAgBo+nkpcXJxZjxs3ThYsWCDZuXmu8VQiw0JkwoQJruN27tzp9hyMp+IeQTcAwFJpJ7MkKzff0nPuP5HlWgcHW1t0RYUFS4uYKEvPCQCAv46n8s0338iNN95Y6ngqOphaaRhPxT2CbgCApQF3r9n/1yfMDuOW7bLlvBvG30rgDQCo9g+8C6IbydJPSi+nv/x8kyS/MEVCw8LkYm6ua79ze+z/PCeJ3XqUef7dh89dNh1RNeyBN0E3AMAyWuAHBGfIhAGNpEk966brysm9KJt37JLunTtIRFioZef96XS2zPrkqOU18wAAVGXAfdvcjyQg+HzlTxbUUBqNekzOfbVCArNO/2d3VD2pf+MQ+SCooXywLbX095f1WhGO/Nqy/qm7akzgTdANALBUSJ0v5dV966y/qyEiG7+14bR1dNqTO6w/MQAAVeBkZq4pe8OutKbsjWohUr9XPRHRpahN/3+pvNwTvSUrt+aUvQTdAABL5Z1NlDkD75eWDWpZds78/Hz5fMvn0u2Wbpb26d5/PFOeWLTfsvMBAFDVtCzTsjc/s53f3HxHfm3TxLymqDlXCgCoEo78aGkR3Vra1b/CsnPm5eVJWnCatK3XVkJCQiw7b+GFc+LIP2HZ+QAAqGr9rosVkW7mYXdESJBl5009es6MpTJnaAdp3ci6Ml3RpxsAAAAA4BfqRYXKfV2aVnrKsEtdPHZWctP3ycVjEXIxsI5H72HKMPeo6QYAWCYnr8CsPRm5tDyycnJlxwmR2INnJCoizLLz7jueadm5AACoDlOGXer+tzw/linD3CPoBgBY2q9MTVxux9RewfLOvq9sOO//NXMDAKC605poDYw9kZmTK6s2bJWBvbpKLQ8feOv5URK/MgAAFvcrE/qVAQDggyIjI6VTp04ej6dy5uRx6dqls6XjqdREBN0AAK/0Kyvv6OWq5ZVR0j7O2sFcAAAA7ETQDQDwivIM5pJ69P8Gc9mzO0IKTzGYCwAA8B8E3QAAr2AwFwAAUBMQdAMAvILBXAAAQE1A0A0A8AoGcwEAADVBoLcTAAAAAABAdUXQDQAAAACATQi6AQAAAACwCUE3AAAAAAA2IegGAAAAAKA6B93z5s2T5s2bS3h4uCQmJsr27dvLPP69994zU83o8R06dJCPP/64ytIKAAAAAIDfBN1Lly6VsWPHytSpU2Xnzp0SHx8v/fv3l+PHj7s9/osvvpARI0bIww8/LF9//bUMGTLELLt3767ytAMAAAAA4NNBd3JysiQlJcno0aOlXbt2Mn/+fDN368KFC90e/9JLL8ntt98uEyZMkLZt28rzzz8vnTp1kr/85S9VnnYAAAAAAMoSLF508eJFSUlJkUmTJrn2BQYGSp8+fWTr1q1u36P7tWa8KK0ZX7Fihdvjc3NzzeKUkZFh1nl5eWaBe857wz1CZZCPYBXykuf3yBdQ9lYM+RxWIB/BKuQl68perwbdJ0+elIKCAmnYsGGx/bq9d+9et+9JT093e7zud2fGjBkybdq0Evs1SNcadZTtww8/5Bah0shHsAp5qXTZ2dlm7XA4vJ7hKHsrh3wOK5CPYBXyUuXLXq8G3VVBa9GL1owfPnzYNGN/5JFHvJouAADscP78ebniiiu8enMpewEANcn5y5S9Xg26Y2JiJCgoSI4dO1Zsv27Hxsa6fY/uL8/xYWFhZnGqVauW/PTTT1K7dm0JCAiw5DqqI22G36RJE3OvoqOjvZ0c+CnyEchLVUefsmuhf9VVV3k941H2VgzfmbAC+QhWIS9ZV/Z6NegODQ2VhIQEWbdunRmBXBUWFprtMWPGuH1P165dzetPPfWUa9/atWvNfk9on/HGjRtbdAXVnwbcBN0gH8FX8J1UNm/XcJeGsrd8yOewAvkIViEvVb7s9Xrzcm36PWrUKOncubN06dJF5s6dK1lZWWY0czVy5EiJi4sz/cPUk08+KT179pQ5c+bIwIEDZcmSJbJjxw55/fXXvXwlAAAAAAD4WNA9fPhwOXHihEyZMsUMhtaxY0dZvXq1a7C0Q4cOmSfkTjfffLO8++678j//8z8yefJkueaaa8ygaO3bt/fiVQAAAAAA4INBt9Km5KU1J9+4cWOJfcOGDTML7O2PN3Xq1GL94QHyEbyF7yTUBORzkI/gS/hOsk6AwxfmFgEAAAAAoBr6T7ttAAAAAABgKYJuAAAAAABsQtANAAAAAIBNCLprsE2bNsmdd95pJnMPCAgwo8AXpd39dVT5Ro0aSUREhPTp00f+/e9/ey29qD755vTp0/LAAw+YeR/r1KkjDz/8sGRmZlbxlaC65p3vvvtOunfvLuHh4dKkSROZOXNmlVwf4AnKXlQU5S98Od9Q9paNoLsG0/nQ4+PjZd68eW5f1x+qf/7zn2X+/Pny5ZdfSlRUlPTv318uXLhQ5WlF9co3+sX9/fffy9q1a2XlypWmQPj1r39dhVeB6pp3MjIypF+/ftKsWTNJSUmRWbNmybPPPiuvv/56lVwjcDmUvagoyl/4ar6h7PWAjl4OaFb44IMPXDeisLDQERsb65g1a5Zr39mzZx1hYWGOxYsXc8NQ4Xzzww8/mPd99dVXrmM++eQTR0BAgOPw4cPc2RrCrrzzyiuvOOrWrevIzc11HfPMM884WrduXUVXBniOshcVRfkLX8o3lL2XR0033EpLS5P09HTTxMTpiiuukMTERNm6dSt3DRXON7rWpkmdO3d2HaPHBwYGmiesqJmsyjt6TI8ePSQ0NNR1jD6xT01NlTNnzlTpNQHlRdmLiqL8hTfzDWXv5RF0wy39B6gaNmxYbL9uO18DKpJvdN2gQYNirwcHB0u9evXIWzWYVXlH1+7OUfQzAF9F2Qs78w7lL+zKN5S9l0fQDQAAAACATQi64VZsbKxZHzt2rNh+3Xa+BlQk3+j6+PHjxV7Pz883I2OSt2ouq/KOrt2do+hnAL6Kshd25h3KX9iVbyh7L4+gG261aNHC/ANat25dsZEJte9G165duWuocL7R9dmzZ83I0k7r16+XwsJC04cINZNVeUeP0VFV8/LyXMfoaKutW7eWunXrVuk1AeVF2YuKovyFN/MNZa8HPBhsDdXU+fPnHV9//bVZNCskJyeb/z948KB5/Y9//KOjTp06jg8//NDx3XffOQYPHuxo0aKFIycnx9tJh5/nm9tvv91xww03OL788kvHli1bHNdcc41jxIgRXrwqVJe8o6OuNmzY0PHggw86du/e7ViyZIkjMjLS8dprr/FHhk+g7IU38w7lb81D2esbCLprsA0bNpgv7UuXUaNGuaYR+P3vf29+wOrUAb1793akpqZ6O9moBvnm1KlTJlCqVauWIzo62jF69GhTKKB6q6q88+233zpuueUWc464uDjzQxTwFZS98GbeofyteSh7fUOA/seTGnEAAAAAAFA+9OkGAAAAAMAmBN0AAAAAANiEoBsAAAAAAJsQdAMAAAAAYBOCbgAAAAAAbELQDQAAAACATQi6AQAAAACwCUE3AAAAAAA2IegGYJuAgABZsWKFLec+cOCAOf8333xjy/kBAPBHlL2A7yHoBvzYiRMn5NFHH5WmTZtKWFiYxMbGSv/+/eXzzz+X6q5JkyZy9OhRad++vbeTAgCoQSh7KXuB8gou9zsA+Ix7771XLl68KG+99ZZcffXVcuzYMVm3bp2cOnVKqrugoCDzkAEAgKpE2UvZC5QXNd2Anzp79qxs3rxZXnzxRenVq5c0a9ZMunTpIpMmTZK77rrLdVxycrJ06NBBoqKiTO3wY489JpmZma7X33zzTalTp46sXLlSWrduLZGRkTJ06FDJzs42wXzz5s2lbt268sQTT0hBQYHrfbr/+eeflxEjRphzx8XFybx588pM808//SS/+MUvzOfVq1dPBg8ebJqJl+bMmTPywAMPyJVXXikRERFyzTXXyBtvvOG2eflDDz1kti9dNm7caF7Pzc2V8ePHm3RqehMTE12vAQBA2UvZC9iFoBvwU7Vq1TKL9pnWgLI0gYGB8uc//1m+//57E0SvX79efve73xU7RgNsPWbJkiWyevVqE4zefffd8vHHH5vlnXfekddee02WLVtW7H2zZs2S+Ph4+frrr2XixIny5JNPytq1a92mIy8vzzR9r127tnlYoE3gNf233367qa135/e//7388MMP8sknn8iePXvk1VdflZiYGLfHvvTSS6a5uXPRtDRo0EDatGljXh8zZoxs3brVXON3330nw4YNM5/973//+7L3GgAARdlL2QtUiAOA31q2bJmjbt26jvDwcMfNN9/smDRpkuPbb78t8z3vvfeeo379+q7tN954w6FfBfv27XPt+81vfuOIjIx0nD9/3rWvf//+Zr9Ts2bNHLfffnuxcw8fPtwxYMAA17ae94MPPjD//8477zhat27tKCwsdL2em5vriIiIcHz66adu03rnnXc6Ro8e7fa1tLQ0c/6vv/66xGvvv/++uSdbtmwx2wcPHnQEBQU5Dh8+XOy43r17m3sGAICnKHspe4HyoqYb8PN+ZUeOHJGPPvrI1NpqDXWnTp1Mk3Gnf/7zn9K7d2/TrFprmR988EHT51trt520SXnLli1d2w0bNjTNx/WJftF9x48fL/b5Xbt2LbGtNdLufPvtt7Jv3z6TBmdNgTYxv3Dhguzfv9/te3SQOK2Z7tixo6md/+KLLy57T7TWXa/xL3/5i3Tr1s3s27Vrl2kaf+2117o+W5fPPvus1M8GAMAdyl7KXqC8GEgN8HPh4eHSt29fs2hz7EceeUSmTp1q+jhrv+dBgwaZ4PUPf/iDCXK3bNkiDz/8sGnSrcG2CgkJKXZO7Qvtbl9hYWGF06n9yBMSEmTRokUlXtM+2+4MGDBADh48aJq4a7N1fXjw+OOPy+zZs90en56ebvqz6z3Qayz62TrwWkpKilkXVfTBAgAAnqDspewFyoOgG6hm2rVr55obW4NMDZTnzJlj+narf/zjH5Z91rZt20pst23b1u2xWgO/dOlS0886Ojra48/QgHzUqFFm6d69u0yYMMFt0K015jowm/bh1sHjirrhhhtMTbfW1Os5AACwEmUvZS9QFpqXA35Km4jfdttt8ve//90MDJaWlibvvfeezJw50wSfqlWrVmYAs5dffll+/PFHMyDa/PnzLUuDDoamn/evf/3LjFyun68DmLmjo5DrIGiaNh1ITdOrzeF1VPSff/7Z7XumTJkiH374oWmWrgPB6QjrpQX1v/nNb8zo6DognM6hqrXeumiNvjYr188fOXKkLF++3Hz29u3bZcaMGbJq1SrL7gcAoHqj7C2OshfwDEE34Ke0WbROe/WnP/1JevToIe3btzfNy5OSkkx/ZqUji2utr04rpq9r024NNK0ybtw42bFjh6lJfuGFF8xn6Qjl7mhT9k2bNknTpk3lnnvuMcGzNgHXGurSar5DQ0PNFGjXX3+9uUZtGq59vN3R/tk6arnWNjRq1Mi1OPuB61RjGnRrmnVqtCFDhshXX31l0gMAgCcoeyl7gYoI0NHUKvROADWaDrT21FNPmQUAAFD2AnCPmm4AAAAAAGxC0A0AAAAAgE1oXg4AAAAAgE2o6QYAAAAAwCYE3QAAAAAA2ISgGwAAAAAAmxB0AwAAAABgE4JuAAAAAABsQtANAAAAAIBNCLoBAAAAALAJQTcAAAAAADYh6AYAAAAAQOzx/wBpO+B0tI/fLQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "for ax, (case, subdf) in zip(axes, tvd_results_df.groupby(\"case\")):\n",
    "    subdf.boxplot(column=\"TVD\", by=\"size\", ax=ax)\n",
    "    ax.set_title(f\"Case: {case}\")\n",
    "    ax.set_xlabel(\"Sample size\")\n",
    "    ax.set_ylabel(\"TVD\")\n",
    "\n",
    "plt.suptitle(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b937c",
   "metadata": {},
   "source": [
    "# Gibbs Sampling\n",
    "\n",
    "As stated before, exact sampling is often computationally infeasible.\n",
    "\n",
    "For a general PGM, we need to use a tractable Markov chain Monte Carlo (MCMC) sampler, such as a Gibbs sampler.\n",
    "\n",
    "A Gibbs sampler simulates a chain of states, each state of the chain is a complete assignment of the rvs in the PGM.\n",
    "To sample one state of this chain, we iterate over the rvs in the PGM in any order (fixed or random) resampling one rv at a time while keeping the others fixed. This means that each time we sample an rv we are certain to have fixed the rvs in its Markov blanket, and hence, resampling this one rv is a tractable operation. \n",
    "\n",
    "If we run the sampler for enough interations, under certain conditions discussed in class, the chain converges to something called the _stationary distribution_. The stationary distribution of a well designed sampler is going to be the joint distribution represented by our PGM. \n",
    "\n",
    "Of course, there are many hyperparameters in this procedure, and we typically have to run it for many iterations. But note that at no point this procedure will create an object that grows intractably large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1f1dc4f5-d2b0-4ea3-8817-ef91a20aa8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampler(\n",
    "    pgm: PGM,   \n",
    "    initial_assignment: dict,\n",
    "    num_iterations: int,\n",
    "    burn_in: int = 0,\n",
    "    thinning: int = 1,\n",
    "    order=None,\n",
    "    shuffle_order=True,\n",
    "    rng=np.random.default_rng(),\n",
    "):   \n",
    "    \"\"\"\n",
    "    Simulate a Gibbs chain for a number of iterations and return a list of samples (each a dict).\n",
    "    (possibly after discarding some samples, depending on burn-in and thinning).\n",
    "    \n",
    "    pgm: the model we are sampling from\n",
    "        (if you need to condition on something, construct a PGM with reduced factors before calling this function)\n",
    "    initial_assignment: a complete assignment of all of the pgm's rvs\n",
    "    num_iterations: how many iterations we run the Gibbs sampler for\n",
    "    burn_in: how many samples we discard from the beginning of the chain\n",
    "    thinning: for some number k, collect every kth sample, discard the rest\n",
    "    order: in which order should we resample rvs\n",
    "    shuffle_order: whether to shuffle the order (uniformly at random) at the beginning of each iteration\n",
    "    rng: a numpy random number generator\n",
    "        (necessary so we can sample from factors; if not provided, we use np.random.default_rng())\n",
    "    \"\"\"    \n",
    "    variables = set(pgm.iternodes())\n",
    "    # validate\n",
    "    assert variables <= set(initial_assignment.keys()), \"initial_assignment must define every variable\"\n",
    "    \n",
    "    if order is None: # fix an order if none is given\n",
    "        order = list(pgm.iternodes())\n",
    "\n",
    "    current_assignment = dict(initial_assignment)  \n",
    "    \n",
    "    samples = []\n",
    "    for j in range(num_iterations):\n",
    "        if shuffle_order:  # shuffle the order if needed\n",
    "            permutation = rng.permutation(len(order))\n",
    "        else:\n",
    "            permutation = np.arange(len(order))\n",
    "        for i in permutation:  # for each rv in order\n",
    "            X = order[i]\n",
    "            del current_assignment[X]  # remove rv from assignment\n",
    "            # find relevant factors\n",
    "            relevant, irrelevant = split_factors(X, pgm.iterfactors())\n",
    "            # reduce the relevant factors (this is fixing the rvs in the rv's Markov Blanket)\n",
    "            relevant = [f.reduce(current_assignment) for f in relevant]\n",
    "            \n",
    "            # compute product of factor for all relevant, reduced factors\n",
    "            prod = functools.reduce(lambda a, b: a.product(b), relevant)\n",
    "            # normalisation gives us the probability distribution over X given MB(X)\n",
    "            P_X = prod.normalize()  \n",
    "                        \n",
    "            # then we can sample an new outcome\n",
    "            # and that gives us a new assignment\n",
    "            current_assignment[X] = P_X.sample(rng=rng)[X]\n",
    "            \n",
    "        # at the end of a complete pass through all rvs, we have a sample\n",
    "        samples.append(dict(current_assignment))\n",
    "\n",
    "    return samples[burn_in:][::thinning]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bbcb29",
   "metadata": {},
   "source": [
    "## Starting assignment\n",
    "\n",
    "The idea of the Gibbs sampler is that we continuously sample one variable, conditioned on the assignment of the other variables as evidence. When starting from some initial assignment, running the gibbs sampling algorithm for long enough, the assignment we get will be as if we sampled from the distribution (under the assumptions discussed in class).\n",
    "\n",
    "\n",
    "There are many ways we can start:\n",
    "- for BNs, we can obtain a _forward sample_ (without evidence)\n",
    "- for MNs, we can start from an arbitrary sample (simply starting from whatever outcome is possible of each and every rv, making this choice independently and arbitrarily at random)\n",
    "- if we had access to data, we could start from an assignment that has been observed in the data\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "404b97ec-1635-46b1-a8f3-cc0ba99585cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_initial_assignment(pgm: PGM, rng=np.random.default_rng()):\n",
    "    \"\"\"\n",
    "    Obtain an initial assignment of the PGM's RVs by any one of the procedures explained above.\n",
    "\n",
    "    pgm: the PGM we are drawing the initial assignment from\n",
    "    rng: a numpy random number generator\n",
    "\n",
    "    The return type is a dictionary representing the assignment.\n",
    "    \"\"\"\n",
    "\n",
    "    assignment = dict()\n",
    "    for X, Val_X in pgm.iterrvs():\n",
    "        id_x = rng.choice(len(Val_X))\n",
    "        assignment[X] = Val_X.outcomes[id_x]\n",
    "    return assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073857c",
   "metadata": {},
   "source": [
    "Now we demo how you can use Gibbs sampling (we demo it using Misconception). We also compare the result to the target distribution (since for this small PGM that can be done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "454606ba-0d83-4810-accc-d8abcfef79c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 'a1', 'B': 'b1', 'C': 'c1', 'D': 'd0'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_initial_assignment(misconception_mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5870f16a-3fe7-46d3-b390-fb1abfef83f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     B   D   A   C  Count  Value\n",
      "0   b1  d0  a0  c1    647  0.647\n",
      "1   b0  d1  a1  c0    170  0.170\n",
      "2   b0  d1  a0  c0     53  0.053\n",
      "3   b0  d0  a0  c0     47  0.047\n",
      "4   b0  d0  a0  c1     39  0.039\n",
      "5   b1  d1  a1  c0     15  0.015\n",
      "6   b1  d0  a1  c1     15  0.015\n",
      "7   b1  d1  a1  c1     11  0.011\n",
      "8   b0  d0  a1  c1      1  0.001\n",
      "9   b1  d0  a0  c0      1  0.001\n",
      "10  b1  d1  a0  c0      1  0.001\n",
      "TVD Misconception(target, gibbs) 0.052911133821356746\n"
     ]
    }
   ],
   "source": [
    "misconception_gibbs_samples_1 = gibbs_sampler(\n",
    "    misconception_mn,\n",
    "    draw_initial_assignment(misconception_mn),\n",
    "    num_iterations=1000,\n",
    ")\n",
    "misconception_gibbs_samples_df = make_samples_df(misconception_gibbs_samples_1)\n",
    "\n",
    "# Note that you can verify earlier answers using this table\n",
    "print(misconception_gibbs_samples_df)\n",
    "print(\"TVD Misconception(target, gibbs)\", tvd(pgm_to_df(misconception_mn, normalize=True), misconception_gibbs_samples_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9274ab-b4cc-4055-8464-f6624323797c",
   "metadata": {},
   "source": [
    "**EXERCISE - Gibbs Sampling**\n",
    "\n",
    "For the medical BN, obtain samples via Gibbs sampling and compare the empirical distribution to the target distribution in terms of TVD in the following cases:\n",
    "\n",
    "1. No evidence\n",
    "2. Evidence: $S=s^1$. Remember that to perform Gibbs sampling conditionally, there is no need to use sum-product VE at all, rather you should simply reduce the factors before starting the chain and your initial state should be compatible with the evidence. \n",
    "\n",
    "For each case above, you should experiment with sample size (10, 100 and 1000 samples). For each experiment (1 and 2) and for each sample size (10, 100, 1000) you should repeat the experiment 20 times so that you can see how TVD varies. Plot the results conveniently (for example, using a boxplot). \n",
    "\n",
    "Finally, discuss what you observe in your experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5bec2beb-d79f-4bd8-8a06-13a0727614e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>size</th>\n",
       "      <th>rep</th>\n",
       "      <th>TVD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_evidence</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.351591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_evidence</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_evidence</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.408669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_evidence</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.436680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no_evidence</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.494814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          case  size  rep       TVD\n",
       "0  no_evidence    10    0  0.351591\n",
       "1  no_evidence    10    1  0.410724\n",
       "2  no_evidence    10    2  0.408669\n",
       "3  no_evidence    10    3  0.436680\n",
       "4  no_evidence    10    4  0.494814"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical_mn = MarkovNetwork(list(medical_bn.iterfactors()))\n",
    "\n",
    "evidence = {'S': 's1'}\n",
    "medical_mn_S1 = MarkovNetwork(\n",
    "    [f.reduce(evidence) for f in medical_bn.iterfactors()]\n",
    ")\n",
    "\n",
    "\n",
    "def run_gibbs_experiment(pgm, label, sizes=(10, 100, 1000), repeats=20, rng=None):\n",
    "    \"\"\"\n",
    "    pgm: PGM (hier een MarkovNetwork) waar we Gibbs op draaien\n",
    "    label: string voor de case (\"no_evidence\" of \"S=s1\")\n",
    "    sizes: sample sizes\n",
    "    repeats: aantal herhalingen per sample size\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for size in sizes:\n",
    "        for rep in range(repeats):\n",
    "            init = draw_initial_assignment(pgm, rng=rng)\n",
    "\n",
    "            burn_in = 100\n",
    "            num_iterations = burn_in + size\n",
    "\n",
    "            samples = gibbs_sampler(\n",
    "                pgm=pgm,\n",
    "                initial_assignment=init,\n",
    "                num_iterations=num_iterations,\n",
    "                burn_in=burn_in,\n",
    "                thinning=1,\n",
    "                shuffle_order=True,\n",
    "                rng=rng,\n",
    "            )\n",
    "\n",
    "            samples_df = make_samples_df(samples)\n",
    "\n",
    "            target_df = pgm_to_df(pgm, normalize=True)\n",
    "\n",
    "            tvd_val = tvd(target_df, samples_df)\n",
    "\n",
    "            records.append({\n",
    "                \"case\": label,\n",
    "                \"size\": size,\n",
    "                \"rep\": rep,\n",
    "                \"TVD\": tvd_val,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Case 1: geen evidence\n",
    "tvd_no_ev = run_gibbs_experiment(medical_mn, \"no_evidence\")\n",
    "\n",
    "# Case 2: evidence S = s1  (we gebruiken medical_mn_S1)\n",
    "tvd_S1   = run_gibbs_experiment(medical_mn_S1, \"S=s1\")\n",
    "\n",
    "tvd_gibbs_df = pd.concat([tvd_no_ev, tvd_S1], ignore_index=True)\n",
    "tvd_gibbs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e0401ca3-856b-44a9-af44-9354dae66179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "for ax, (case, subdf) in zip(axes, tvd_gibbs_df.groupby(\"case\")):\n",
    "    subdf.boxplot(column=\"TVD\", by=\"size\", ax=ax)\n",
    "    ax.set_title(f\"Gibbs: {case}\")\n",
    "    ax.set_xlabel(\"Sample size\")\n",
    "    ax.set_ylabel(\"TVD\")\n",
    "\n",
    "plt.suptitle(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd8894",
   "metadata": {},
   "source": [
    "## Mixing / Convergence\n",
    "\n",
    "When we start a Gibbs sampler from an initial assignment, the first few samples are typically not representative of the target distribution. The sampler needs time to \"forget\" its starting point and explore the state space according to the PGM's joint distribution. This process of transitioning from the initial state to sampling from the stationary distribution is called _mixing_ or _convergence_.\n",
    "\n",
    "The number of initial samples that should be discarded (called the _burn-in_ period) depends on how quickly the chain converges. In practice, we often need to run the sampler for many iterations before the samples become representative of the target distribution.\n",
    "\n",
    "\n",
    "### Approaches to Assess Convergence\n",
    "\n",
    "There are two main approaches to assess convergence, each with different applicability:\n",
    "\n",
    "1. **Comparing to the target distribution**: For small, tractable PGMs where we can compute the exact joint distribution, we can directly measure how close our Gibbs samples are to the target distribution using metrics like Total Variation Distance (TVD). As we run more iterations, the empirical distribution of our samples should get closer to the true distribution, and the TVD should decrease toward 0. This is the gold standard when available, but it's only feasible for toy examples.\n",
    "\n",
    "In fact, you have (more or less) already done this in an earlier exercise when we were working with the medical BN!\n",
    "\n",
    "\n",
    "2. **Multiple chain diagnostics**: For larger PGMs where computing the exact distribution is intractable, we use multiple chain diagnostics. The idea is to run several independent chains (each starting from a different initial assignment) and check if they are sampling from the same distribution. If chains have converged, they should all be exploring the same region of the state space, and statistics computed across chains should be similar. If chains haven't converged, they might be stuck in different modes or regions, leading to different statistics. By monitoring how R-hat changes over iterations and for different burn-in periods, we can assess whether the chains have mixed and determine an appropriate burn-in period.\n",
    "\n",
    "\n",
    "***R-hat (Gelman-Rubin statistic)***\n",
    "\n",
    "One of the most common diagnostics is the **R-hat** (or $\\hat{R}$) statistic, also known as the Gelman-Rubin statistic. It compares the variance within chains to the variance between chains.\n",
    "\n",
    "- **R-hat  1.00-1.05**: Good mixing (usually adequate for PGM use)\n",
    "- **R-hat > 1.1**: Chains not yet mixing; probably stuck in different energy basins\n",
    "- **R-hat >> 1.1**: Very poor mixing or chains starting from very different modes\n",
    "\n",
    "We'll use the log unnormalized probability of each sample as the statistic to monitor. This is a scalar value that summarizes each sample and should converge across chains if mixing is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d708697f-3fb1-45a7-b865-116f339cefe5",
   "metadata": {},
   "source": [
    "***Multiple Independent Chains***\n",
    "\n",
    "To obtain multiple, independent chains, we typically simply initialise a number of Gibbs chains at different initial states. \n",
    "\n",
    "It's in fact possible to indeed simulate these chains in parallel. We wrote some helper code for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21048670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chain(sampler_fn, seed, init, args, kwargs):\n",
    "    \"\"\"Run one chain calling sampler_fn\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return sampler_fn(*args, rng=rng, **kwargs, initial_assignment=init)\n",
    "\n",
    "def run_chains_joblib(sampler_fn, inits: list, *args, **kwargs):\n",
    "    \"\"\"Run K chains in paralell, using sampler_fn, we determine the number K by checking the size of the inits list.\"\"\"\n",
    "    K = len(inits)\n",
    "    master_rng = np.random.default_rng()\n",
    "    seeds = master_rng.integers(0, 2**63-1, size=K)\n",
    "    \n",
    "    results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "        delayed(run_chain)(sampler_fn, seed, init, args, kwargs)\n",
    "        for seed, init in zip(seeds, inits)\n",
    "    )\n",
    "    return np.stack(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a3f40-8b5a-48a0-aa84-f8e6a2cbd383",
   "metadata": {},
   "source": [
    "Here's a demo using the Misconception example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38420495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chains for the Misconception example: 4\n"
     ]
    }
   ],
   "source": [
    "# draw 4 chains\n",
    "chains = run_chains_joblib(\n",
    "    gibbs_sampler,\n",
    "    [draw_initial_assignment(misconception_mn) for _ in range(4)], # 4 initialisers\n",
    "    pgm=misconception_mn,\n",
    "    num_iterations=100,\n",
    "    burn_in=0,\n",
    ")\n",
    "print(\"Chains for the Misconception example:\", len(chains))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd460b6-a483-4428-92fc-35fa67220e12",
   "metadata": {},
   "source": [
    "To evaluate the chains for mixing/convergence, we need to obtain sample statistics. Let's use the log unnormalised probability of the samples as our statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527d8cc-4161-47b2-b37c-b0595ab0097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rhat 1.1151032521382302\n"
     ]
    }
   ],
   "source": [
    "# computes chain statistics -- we are using log unnormalised probability\n",
    "log_uprobs = np.array([[misconception_mn.evaluate(x) for x in chain] for chain in chains])\n",
    "# And now we can compute R-hat\n",
    "print('rhat', rhat_split(log_uprobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330bc53-044a-4a65-b7f1-0fa3d6d74194",
   "metadata": {},
   "source": [
    "When stochasticity is involved, we should never draw conclusions from single experiments. Let's do this a number of times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00eff7-b96d-4b3d-8cb6-b05d18adedb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Experiment    R-hat\n",
      "------------  -------\n",
      "           0  1.15724\n",
      "           1  1.1\n",
      "           2  1.10215\n",
      "           3  1.08268\n",
      "           4  1.22548\n",
      "           5  1.22032\n",
      "           6  1.3237\n",
      "           7  1.14877\n",
      "           8  1.04139\n",
      "           9  1.05527\n",
      "Mean R-hat 1.145700127560348\n"
     ]
    }
   ],
   "source": [
    "rhat_experiments = []\n",
    "for i in range(10):\n",
    "    # draw 4 chains\n",
    "    chains = run_chains_joblib(\n",
    "        gibbs_sampler,\n",
    "        [draw_initial_assignment(misconception_mn) for _ in range(4)], # 4 initialisers\n",
    "        pgm=misconception_mn,\n",
    "        num_iterations=100,\n",
    "        burn_in=0,\n",
    "    )\n",
    "    # computes chain statistics -- we are using log unnormalised probability\n",
    "    log_uprobs = np.array([[misconception_mn.evaluate(x) for x in chain] for chain in chains])\n",
    "    # And now we can compute R-hat\n",
    "    rhat_experiments.append([i, rhat_split(log_uprobs)])\n",
    "print(tabulate(rhat_experiments, headers=[\"Experiment\", \"R-hat\"]))\n",
    "print(\"Mean R-hat\", np.array(rhat_experiments)[:,1].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200f3a90-5a4b-4ca5-9551-3cdfee2c1415",
   "metadata": {},
   "source": [
    "It looks like with 100 samples, we should suspect the chains have not yet converged. \n",
    "We can sample longer and/or discard some of the earlier samples (using `burn_in`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8510a3e0",
   "metadata": {},
   "source": [
    "**EXERCISE - Studying Convergence**\n",
    "\n",
    "\n",
    "For the medical BN, study the setting with evidence $S=s^1$. Our goal is to determine good choices for `num_iterations` and `burn_in`, and we will do so using R-hat estimated on the basis of 4 chains. As we did above, for each configuration of `num_iterations` and `burn_in`, we should perform the experiment 10 times, so we can be more confident on our findings. \n",
    "\n",
    "For `num_iterations` use [100, 200, 400, 800, 1600]. For `burn_in` you should use `num_iterations//10`, `num_iterations//5` and `num_iterations//2`. \n",
    "\n",
    "* Part 1. Display average R-hat (across the 10 repetitions of each setting, that is) in a plot where we can compare the various settings of `num_iterations` and `burn_in`. Based on your findings, motivate a choice of `num_iterations` and `burn_in` with a rationale based on average r-hat values. If some settings appear to you as roughly as good, motivate which is to be preferred, for example, from a computational standpoint.\n",
    "* Part 2. For the setting you chose, compare the empirical distribution to actual target distribution (that is, to a table view of the target distribution) in terms of TVD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eaa9ae-9592-4dc2-9521-3a58c1d2c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70946e-57e2-495c-9bc4-73075d64c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e6b36",
   "metadata": {},
   "source": [
    "## Expectations\n",
    "\n",
    "Often, we are not interested in the underlying distribution or the samples themselves, but in quantitative metrics that depend on them.\n",
    "\n",
    "In a medical setting, for example, we might want to estimate a patients suffering (e.g., the expected number of symptoms), expected treatment costs, or expected duration of treatment. These quantities are functions of assignments of the model.\n",
    "\n",
    "Formally, if a model $P_\\Phi$ assigns a probability to each possible outcome $x \\in \\text{Val}(X)$ of an rv $X$, and a function \\($f(x)$\\) maps that outcome to the value we care about, then the expectation is\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[f(X)]\n",
    "=\n",
    "\\sum_{x \\in \\text{Val}(X)} f(x)\\, P_\\Phi(x)\n",
    "$$\n",
    "\n",
    "This is the **exact** expectation: a weighted sum over *all* assignments.  \n",
    "However, for most realistic models this summation is intractable because the number of assignments grows exponentially in the number of variables.\n",
    "\n",
    "To avoid this, we approximate the expectation using sampling. If we could draw $M$ independent samples $x_i \\sim P_\\Phi(X)$, we could estimate the expectation as  \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{}[f(X)]\n",
    "\\;\\approx\\;\n",
    "\\hat{\\mathbb{E}}[f(X)]\n",
    "=\n",
    "\\frac{1}{M} \\sum_{i=1}^M f(x[i]).\n",
    "$$\n",
    "\n",
    "In practice, exact independent samples are usually unavailable. Instead, we generate samples using **Gibbs sampling**, which produces a *Markov chain* whose stationary distribution is $P_\\Phi(X)$. The samples in this chain are correlated and not independent, but the same estimator above is still **consistent**: as the chain gets long enough (after burn-in and with sufficient thinning if desired), the average converges to the true expected value.\n",
    "\n",
    "This makes sampling a practical and scalable alternative when exact computation is impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a3c87-9e9b-4cb5-bc6b-747a8abf22c0",
   "metadata": {},
   "source": [
    "**EXERCISE  Computing Expectations**\n",
    "\n",
    "Complete the functions below to compute expectations from Gibbs samples. The function takes the generated chain of samples, applies a function to them (e.g. counting symptoms, evaluating costs, computing treatment time), and returns the average value.  \n",
    "\n",
    "Then, for the medical BN, report\n",
    "* Expected number of symptoms\n",
    "* Expected treatment cost\n",
    "* Expected recovery time\n",
    "1) sampling _without_ evidence, and 2) sampling _given_ $S=s^1$.\n",
    "\n",
    "For this experiment you can use `num_iterations=5000` and `burn_in=100`. You may perform this experiment using a single chain. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12154c-c648-44ce-95d1-5a933aa7d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_n_symptoms(samples: list):\n",
    "    \"\"\"\n",
    "    Compute the expected number of symptoms from Gibbs samples.\n",
    "    \n",
    "    Symptoms are: ST (Sore Throat), F (Fever), C (Cough), W (Wheezing)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : list of dict\n",
    "        List of sample dictionaries from Gibbs sampling\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    expected : float\n",
    "        Expected number of symptoms (0-4)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement this\")\n",
    "\n",
    "\n",
    "def expected_treatment_cost(samples: list):\n",
    "    \"\"\"\n",
    "    Compute the expected treatment cost from Gibbs samples.\n",
    "    \n",
    "    Cost structure:\n",
    "    - Influenza (I): 50\n",
    "    - Sore Throat (ST): 20\n",
    "    - Fever (F): 30\n",
    "    - Bronchitis (B): 100\n",
    "    - Cough (C): 25\n",
    "    - Wheezing (W): 40\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : list of dict\n",
    "        List of sample dictionaries from Gibbs sampling\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    expected_cost : float\n",
    "        Expected treatment cost in euros\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement this\")\n",
    "\n",
    "\n",
    "def expected_recovery_time(samples: list):\n",
    "    \"\"\"\n",
    "    Compute the expected recovery time (in days) from Gibbs samples.\n",
    "    \n",
    "    Recovery time structure:\n",
    "    - Influenza (I): 7 days\n",
    "    - Sore Throat (ST): 3 days\n",
    "    - Fever (F): 2 days\n",
    "    - Bronchitis (B): 10 days\n",
    "    - Cough (C): 5 days\n",
    "    - Wheezing (W): 4 days\n",
    "    \n",
    "    Note: Recovery time is the maximum of all present conditions (conditions overlap).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    samples : list of dict\n",
    "        List of sample dictionaries from Gibbs sampling\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    expected_time : float\n",
    "        Expected recovery time in days\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement this\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840bae8d-30a7-4687-9881-18ba7444f287",
   "metadata": {},
   "source": [
    "This is to help you test the expectations _without evidence_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047f77d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "TODO: implement this",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m samples = gibbs_sampler(\n\u001b[32m      2\u001b[39m     medical_bn,\n\u001b[32m      3\u001b[39m     draw_initial_assignment(medical_bn),\n\u001b[32m      4\u001b[39m     num_iterations=\u001b[32m5000\u001b[39m,\n\u001b[32m      5\u001b[39m     burn_in=\u001b[32m100\u001b[39m,\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m np.isclose(\u001b[43mexpected_n_symptoms\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m, \u001b[32m0.6\u001b[39m, rtol=\u001b[32m0.3\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m np.isclose(expected_treatment_cost(samples), \u001b[32m50.0\u001b[39m, rtol=\u001b[32m0.3\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m np.isclose(expected_recovery_time(samples), \u001b[32m3.0\u001b[39m, rtol=\u001b[32m0.3\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mexpected_n_symptoms\u001b[39m\u001b[34m(samples)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexpected_n_symptoms\u001b[39m(samples: \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Compute the expected number of symptoms from Gibbs samples.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[33;03m        Expected number of symptoms (0-4)\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTODO: implement this\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: TODO: implement this"
     ]
    }
   ],
   "source": [
    "samples = gibbs_sampler(\n",
    "    medical_bn,\n",
    "    draw_initial_assignment(medical_bn),\n",
    "    num_iterations=5000,\n",
    "    burn_in=100,\n",
    ")\n",
    "assert np.isclose(expected_n_symptoms(samples), 0.6, rtol=0.3)\n",
    "assert np.isclose(expected_treatment_cost(samples), 50.0, rtol=0.3)\n",
    "assert np.isclose(expected_recovery_time(samples), 3.0, rtol=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225b1ad",
   "metadata": {},
   "source": [
    "# Use of AI Tools\n",
    "\n",
    "By submitting this notebook for grading you testify that:\n",
    "\n",
    "* AI did not draft an earlier version of your work.\n",
    "* You did not use AI-powered code completion.\n",
    "* You did not implement algorithms suggested by an AI tool.\n",
    "* AI did not revise a version of your work.\n",
    "* You did not implement suggestions made by an AI tool.\n",
    "\n",
    "\n",
    "_You_ in the sentences above refers to you and all your team members.\n",
    "_AI_ refers to LM-based tools and assistants (e.g., ChatGPT, Gemini, UvA AI chat, etc.).\n",
    "\n",
    "If you did make use of an AI tool, you should describe the uses you made of it below. Or indicate that no such tool was used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cca34f",
   "metadata": {},
   "source": [
    "**TYPE YOUR STATEMENT HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adac596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
